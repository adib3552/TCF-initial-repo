{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13400435,"sourceType":"datasetVersion","datasetId":8503836}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# From: gluonts/src/gluonts/time_feature/_base.py\n# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\").\n# You may not use this file except in compliance with the License.\n# A copy of the License is located at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# or in the \"license\" file accompanying this file. This file is distributed\n# on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n# express or implied. See the License for the specific language governing\n# permissions and limitations under the License.\n\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\n\n\nclass TimeFeature:\n    def __init__(self):\n        pass\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        pass\n\n    def __repr__(self):\n        return self.__class__.__name__ + \"()\"\n\n\nclass SecondOfMinute(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.second / 59.0 - 0.5\n\n\nclass MinuteOfHour(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.minute / 59.0 - 0.5\n\n\nclass HourOfDay(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.hour / 23.0 - 0.5\n\n\nclass DayOfWeek(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.dayofweek / 6.0 - 0.5\n\n\nclass DayOfMonth(TimeFeature):\n    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.day - 1) / 30.0 - 0.5\n\n\nclass DayOfYear(TimeFeature):\n    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.dayofyear - 1) / 365.0 - 0.5\n\n\nclass MonthOfYear(TimeFeature):\n    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.month - 1) / 11.0 - 0.5\n\n\nclass WeekOfYear(TimeFeature):\n    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.isocalendar().week - 1) / 52.0 - 0.5\n\n\ndef time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n    \"\"\"\n    Returns a list of time features that will be appropriate for the given frequency string.\n    Parameters\n    ----------\n    freq_str\n        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n    \"\"\"\n\n    features_by_offsets = {\n        offsets.YearEnd: [],\n        offsets.QuarterEnd: [MonthOfYear],\n        offsets.MonthEnd: [MonthOfYear],\n        offsets.Week: [DayOfMonth, WeekOfYear],\n        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Minute: [\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n        offsets.Second: [\n            SecondOfMinute,\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n    }\n\n    offset = to_offset(freq_str)\n\n    for offset_type, feature_classes in features_by_offsets.items():\n        if isinstance(offset, offset_type):\n            return [cls() for cls in feature_classes]\n\n    supported_freq_msg = f\"\"\"\n    Unsupported frequency {freq_str}\n    The following frequencies are supported:\n        Y   - yearly\n            alias: A\n        M   - monthly\n        W   - weekly\n        D   - daily\n        B   - business days\n        H   - hourly\n        T   - minutely\n            alias: min\n        S   - secondly\n    \"\"\"\n    raise RuntimeError(supported_freq_msg)\n\n\ndef time_features(dates, freq='h'):\n    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:43:34.039620Z","iopub.execute_input":"2025-10-17T04:43:34.039834Z","iopub.status.idle":"2025-10-17T04:43:34.454019Z","shell.execute_reply.started":"2025-10-17T04:43:34.039817Z","shell.execute_reply":"2025-10-17T04:43:34.453221Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n\n\n\nclass Dataset_Custom(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTh1.csv',\n                 target='OT', scale=True, timeenc=0, freq='h'):\n        # size [seq_len, label_len, pred_len]\n        # info\n        if size == None:\n            self.seq_len = 24 * 4 * 4\n            self.label_len = 24 * 4\n            self.pred_len = 24 * 4\n        else:\n            self.seq_len = size[0]\n            self.label_len = size[1]\n            self.pred_len = size[2]\n        # init\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n\n        self.features = features\n        self.target = target\n        self.scale = scale\n        self.timeenc = timeenc\n        self.freq = freq\n\n        self.root_path = root_path\n        self.data_path = data_path\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        df_raw = pd.read_csv(os.path.join(self.root_path,\n                                          self.data_path))\n\n        '''\n        df_raw.columns: ['date', ...(other features), target feature]\n        '''\n        cols = list(df_raw.columns)\n        cols.remove(self.target)\n        cols.remove('date')\n        df_raw = df_raw[['date'] + cols + [self.target]]\n        num_train = int(len(df_raw) * 0.7)\n        num_test = int(len(df_raw) * 0.2)\n        num_vali = len(df_raw) - num_train - num_test\n        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n        border2s = [num_train, num_train + num_vali, len(df_raw)]\n        border1 = border1s[self.set_type]\n        border2 = border2s[self.set_type]\n\n        if self.features == 'M' or self.features == 'MS':\n            cols_data = df_raw.columns[1:]\n            df_data = df_raw[cols_data]\n        elif self.features == 'S':\n            df_data = df_raw[[self.target]]\n\n        if self.scale:\n            train_data = df_data[border1s[0]:border2s[0]]\n            self.scaler.fit(train_data.values)\n            data = self.scaler.transform(df_data.values)\n        else:\n            data = df_data.values\n\n        df_stamp = df_raw[['date']][border1:border2]\n        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n        if self.timeenc == 0:\n            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n            data_stamp = df_stamp.drop(['date'], axis=1).values\n        elif self.timeenc == 1:\n            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n            data_stamp = data_stamp.transpose(1, 0)\n\n        self.data_x = data[border1:border2]\n        self.data_y = data[border1:border2]\n        self.data_stamp = data_stamp\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return len(self.data_x) - self.seq_len - self.pred_len + 1\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\nclass Dataset_Solar(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTh1.csv',\n                 target='OT', scale=True, timeenc=0, freq='h'):\n        # size [seq_len, label_len, pred_len]\n        # info\n        self.seq_len = size[0]\n        self.label_len = size[1]\n        self.pred_len = size[2]\n        # init\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n\n        self.features = features\n        self.target = target\n        self.scale = scale\n        self.timeenc = timeenc\n        self.freq = freq\n\n        self.root_path = root_path\n        self.data_path = data_path\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        df_raw = []\n        with open(os.path.join(self.root_path, self.data_path), \"r\", encoding='utf-8') as f:\n            for line in f.readlines():\n                line = line.strip('\\n').split(',')\n                data_line = np.stack([float(i) for i in line[1:]])\n                df_raw.append(data_line)\n        df_raw = np.stack(df_raw, 0)\n        df_raw = pd.DataFrame(df_raw)\n\n        num_train = int(len(df_raw) * 0.7)\n        num_test = int(len(df_raw) * 0.2)\n        num_valid = int(len(df_raw) * 0.1)\n        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n        border2s = [num_train, num_train + num_valid, len(df_raw)]\n        border1 = border1s[self.set_type]\n        border2 = border2s[self.set_type]\n\n        df_data = df_raw.values\n\n        if self.scale:\n            train_data = df_data[border1s[0]:border2s[0]]\n            self.scaler.fit(train_data)\n            data = self.scaler.transform(df_data)\n        else:\n            data = df_data\n\n        self.data_x = data[border1:border2]\n        self.data_y = data[border1:border2]\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n        seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return len(self.data_x) - self.seq_len - self.pred_len + 1\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\n\nclass Dataset_ETT_hour(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTh1.csv',\n                 target='OT', scale=True, timeenc=0, freq='h'):\n        # size [seq_len, label_len, pred_len]\n        # info\n        if size == None:\n            self.seq_len = 24 * 4 * 4\n            self.label_len = 24 * 4\n            self.pred_len = 24 * 4\n        else:\n            self.seq_len = size[0]\n            self.label_len = size[1]\n            self.pred_len = size[2]\n        # init\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n\n        self.features = features\n        self.target = target\n        self.scale = scale\n        self.timeenc = timeenc\n        self.freq = freq\n\n        self.root_path = root_path\n        self.data_path = data_path\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        df_raw = pd.read_csv(os.path.join(self.root_path,\n                                          self.data_path))\n\n        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n        border1 = border1s[self.set_type]\n        border2 = border2s[self.set_type]\n\n        if self.features == 'M' or self.features == 'MS':\n            cols_data = df_raw.columns[1:]\n            df_data = df_raw[cols_data]\n        elif self.features == 'S':\n            df_data = df_raw[[self.target]]\n\n        if self.scale:\n            train_data = df_data[border1s[0]:border2s[0]]\n            self.scaler.fit(train_data.values)\n            data = self.scaler.transform(df_data.values)\n        else:\n            data = df_data.values\n\n        df_stamp = df_raw[['date']][border1:border2]\n        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n        if self.timeenc == 0:\n            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n            data_stamp = df_stamp.drop(['date'], axis=1).values\n        elif self.timeenc == 1:\n            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n            data_stamp = data_stamp.transpose(1, 0)\n\n        self.data_x = data[border1:border2]\n        self.data_y = data[border1:border2]\n        self.data_stamp = data_stamp\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return len(self.data_x) - self.seq_len - self.pred_len + 1\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\nclass Dataset_ETT_minute(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTm1.csv',\n                 target='OT', scale=True, timeenc=0, freq='t'):\n        # size [seq_len, label_len, pred_len]\n        # info\n        if size == None:\n            self.seq_len = 24 * 4 * 4\n            self.label_len = 24 * 4\n            self.pred_len = 24 * 4\n        else:\n            self.seq_len = size[0]\n            self.label_len = size[1]\n            self.pred_len = size[2]\n        # init\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n\n        self.features = features\n        self.target = target\n        self.scale = scale\n        self.timeenc = timeenc\n        self.freq = freq\n\n        self.root_path = root_path\n        self.data_path = data_path\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        df_raw = pd.read_csv(os.path.join(self.root_path,\n                                          self.data_path))\n\n        border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]\n        border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n        border1 = border1s[self.set_type]\n        border2 = border2s[self.set_type]\n\n        if self.features == 'M' or self.features == 'MS':\n            cols_data = df_raw.columns[1:]\n            df_data = df_raw[cols_data]\n        elif self.features == 'S':\n            df_data = df_raw[[self.target]]\n\n        if self.scale:\n            train_data = df_data[border1s[0]:border2s[0]]\n            self.scaler.fit(train_data.values)\n            data = self.scaler.transform(df_data.values)\n        else:\n            data = df_data.values\n\n        df_stamp = df_raw[['date']][border1:border2]\n        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n        if self.timeenc == 0:\n            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n            data_stamp = df_stamp.drop(['date'], axis=1).values\n        elif self.timeenc == 1:\n            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n            data_stamp = data_stamp.transpose(1, 0)\n\n        self.data_x = data[border1:border2]\n        self.data_y = data[border1:border2]\n        self.data_stamp = data_stamp\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return len(self.data_x) - self.seq_len - self.pred_len + 1\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\n\nclass Dataset_PEMS(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTh1.csv',\n                 target='OT', scale=True, timeenc=0, freq='h'):\n        # size [seq_len, label_len, pred_len]\n        # info\n        self.seq_len = size[0]\n        self.label_len = size[1]\n        self.pred_len = size[2]\n        # init\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n\n        self.features = features\n        self.target = target\n        self.scale = scale\n        self.timeenc = timeenc\n        self.freq = freq\n\n        self.root_path = root_path\n        self.data_path = data_path\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        data_file = os.path.join(self.root_path, self.data_path)\n        data = np.load(data_file, allow_pickle=True)\n        data = data['data'][:, :, 0]\n\n        train_ratio = 0.6\n        valid_ratio = 0.2\n        train_data = data[:int(train_ratio * len(data))]\n        valid_data = data[int(train_ratio * len(data)): int((train_ratio + valid_ratio) * len(data))]\n        test_data = data[int((train_ratio + valid_ratio) * len(data)):]\n        total_data = [train_data, valid_data, test_data]\n        data = total_data[self.set_type]\n\n        if self.scale:\n            self.scaler.fit(train_data)\n            data = self.scaler.transform(data)\n\n        df = pd.DataFrame(data)\n        df = df.fillna(method='ffill', limit=len(df)).fillna(method='bfill', limit=len(df)).values\n\n        self.data_x = df\n        self.data_y = df\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n        seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return len(self.data_x) - self.seq_len - self.pred_len + 1\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:43:34.455436Z","iopub.execute_input":"2025-10-17T04:43:34.455826Z","iopub.status.idle":"2025-10-17T04:43:38.690719Z","shell.execute_reply.started":"2025-10-17T04:43:34.455805Z","shell.execute_reply":"2025-10-17T04:43:38.690128Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom math import sqrt\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n        super(EncoderLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.attention = attention\n        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n\n    def forward(self, x, attn_mask=None, tau=None, delta=None):\n        new_x, attn = self.attention(\n            x, x, x,\n            attn_mask=attn_mask,\n            tau=tau, delta=delta\n        )\n        x = x + self.dropout(new_x)\n\n        y = x = self.norm1(x)\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n\n        return self.norm2(x + y), attn\n\n\nclass Encoder(nn.Module):\n    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n        super(Encoder, self).__init__()\n        self.attn_layers = nn.ModuleList(attn_layers)\n        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n        self.norm = norm_layer\n\n    def forward(self, x, attn_mask=None, tau=None, delta=None):\n        # x [B, L, D]\n        attns = []\n        if self.conv_layers is not None:\n            for i, (attn_layer, conv_layer) in enumerate(zip(self.attn_layers, self.conv_layers)):\n                delta = delta if i == 0 else None\n                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                x = conv_layer(x)\n                attns.append(attn)\n            x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n            attns.append(attn)\n        else:\n            for attn_layer in self.attn_layers:\n                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                attns.append(attn)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        return x, attns\n\nclass FullAttention(nn.Module):\n    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n        super(FullAttention, self).__init__()\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n\n    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n        B, L, H, E = queries.shape\n        _, S, _, D = values.shape\n        scale = self.scale or 1. / sqrt(E)\n\n        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n\n        if self.mask_flag:\n            if attn_mask is None:\n                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n\n            scores.masked_fill_(attn_mask.mask, -np.inf)\n\n        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n\n        if self.output_attention:\n            return (V.contiguous(), A)\n        else:\n            return (V.contiguous(), None)\nclass AttentionLayer(nn.Module):\n    def __init__(self, attention, d_model, n_heads, d_keys=None,\n                 d_values=None):\n        super(AttentionLayer, self).__init__()\n\n        d_keys = d_keys or (d_model // n_heads)\n        d_values = d_values or (d_model // n_heads)\n\n        self.inner_attention = attention\n        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n        self.n_heads = n_heads\n\n    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n        B, L, _ = queries.shape\n        _, S, _ = keys.shape\n        H = self.n_heads\n\n        queries = self.query_projection(queries).view(B, L, H, -1)\n        keys = self.key_projection(keys).view(B, S, H, -1)\n        values = self.value_projection(values).view(B, S, H, -1)\n\n        out, attn = self.inner_attention(\n            queries,\n            keys,\n            values,\n            attn_mask,\n            tau=tau,\n            delta=delta\n        )\n        out = out.view(B, L, -1)\n\n        return self.out_projection(out), attn\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:43:38.691460Z","iopub.execute_input":"2025-10-17T04:43:38.691829Z","iopub.status.idle":"2025-10-17T04:43:38.707393Z","shell.execute_reply.started":"2025-10-17T04:43:38.691811Z","shell.execute_reply":"2025-10-17T04:43:38.706640Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils import weight_norm\nimport math\n\nclass DataEmbedding_inverted(nn.Module):\n    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n        super(DataEmbedding_inverted, self).__init__()\n        self.value_embedding = nn.Linear(c_in, d_model)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, x_mark):\n        x = x.permute(0, 2, 1)\n        # x: [Batch Variate Time]\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            # the potential to take covariates (e.g. timestamps) as tokens\n            x = self.value_embedding(torch.cat([x, x_mark.permute(0, 2, 1)], 1)) \n        # x: [Batch Variate d_model]\n        return self.dropout(x)\n\n\nclass FlattenHead(nn.Module):\n    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n        super().__init__()\n        self.n_vars = n_vars\n        self.flatten = nn.Flatten(start_dim=-2)\n        self.linear = nn.Linear(nf, target_window)\n        self.dropout = nn.Dropout(head_dropout)\n\n    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n        x = self.flatten(x)\n        x = self.linear(x)\n        x = self.dropout(x)\n        return x\n\n\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEmbedding, self).__init__()\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model).float()\n        pe.require_grad = False\n\n        position = torch.arange(0, max_len).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float()\n                    * -(math.log(10000.0) / d_model)).exp()\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return self.pe[:, :x.size(1)]\n\nclass ChannelWiseConv(nn.Module):\n    def __init__(self,c_in,kernel_window, stride=1,padding=3,bias=True):\n        super(ChannelWiseConv,self).__init__()\n        padding = (kernel_window - 1) // 2 if kernel_window % 2 == 1 else kernel_window // 2\n        self.conv = nn.Conv1d(\n            in_channels=c_in, \n            out_channels=c_in, \n            kernel_size=kernel_window, \n            stride=stride, \n            padding=padding, \n            groups=c_in, \n            bias=bias\n        )\n\n    def forward(self, x):\n        # x shape: (batch_size, channels, sequence_length)\n        return self.conv(x)\n\nclass EnEmbedding(nn.Module):\n    def __init__(self, n_vars, d_model, patch_len, kernel_size, dropout):\n        super(EnEmbedding, self).__init__()\n        # Patching\n        self.patch_len = patch_len\n        self.pre_conv = ChannelWiseConv(n_vars, kernel_size)\n\n        self.value_embedding = nn.Linear(patch_len, d_model, bias=False)\n        self.glb_token = nn.Parameter(torch.randn(1, n_vars, 1, d_model))\n        self.position_embedding = PositionalEmbedding(d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # do patching\n        batch,n_vars,temp = x.shape\n        glb = self.glb_token.repeat((x.shape[0], 1, 1, 1))\n        x = self.pre_conv(x)\n        x = x.unfold(dimension=-1, size=self.patch_len, step=self.patch_len)\n        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n        # Input encoding\n        x = self.value_embedding(x) + self.position_embedding(x)\n        x = torch.reshape(x, (-1, n_vars, x.shape[-2], x.shape[-1]))\n        x = torch.cat([x, glb], dim=2)\n        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n        return self.dropout(x),batch,n_vars,x.shape[2]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:43:38.708195Z","iopub.execute_input":"2025-10-17T04:43:38.708548Z","iopub.status.idle":"2025-10-17T04:43:38.731908Z","shell.execute_reply.started":"2025-10-17T04:43:38.708525Z","shell.execute_reply":"2025-10-17T04:43:38.731379Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, self_attention, d_model, d_core, d_ff=None, dropout=0.1, activation=\"relu\"):\n        super(EncoderLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.self_attention = self_attention\n        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n        self.gen1 = nn.Linear(d_model, d_model)\n        self.gen2 = nn.Linear(d_model, d_core)\n        self.gen3 = nn.Linear(d_model + d_core, d_model)\n        self.gen4 = nn.Linear(d_model, d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n\n    def forward(self, x, B, N, D, x_mask=None, tau=None, delta=None):\n        x = x + self.dropout(self.self_attention(\n            x, x, x,\n            attn_mask=x_mask,\n            tau=tau, delta=None\n        )[0])\n        x = self.norm1(x)\n        x_glb_ori = x[:, -1, :].unsqueeze(1)\n        x_glb = torch.reshape(x_glb_ori, (B, -1, D))\n        batch_size, channels, d_model = x_glb.shape\n        combined_mean = F.gelu(self.gen1(x_glb))\n        combined_mean = self.gen2(combined_mean)\n        # stochastic pooling\n        if self.training:\n            ratio = F.softmax(combined_mean, dim=1)\n            ratio = ratio.permute(0, 2, 1)\n            ratio = ratio.reshape(-1, channels)\n            indices = torch.multinomial(ratio, 1)\n            indices = indices.view(batch_size, -1, 1).permute(0, 2, 1)\n            combined_mean = torch.gather(combined_mean, 1, indices)\n            combined_mean = combined_mean.repeat(1, channels, 1)\n        else:\n            weight = F.softmax(combined_mean, dim=1)\n            combined_mean = torch.sum(combined_mean * weight, dim=1, keepdim=True).repeat(1, channels, 1)\n\n        # mlp fusion\n        combined_glb_cat = torch.cat([x_glb, combined_mean], -1)\n        combined_glb_cat = F.gelu(self.gen3(combined_glb_cat))\n        combined_glb_cat = self.gen4(combined_glb_cat)\n        combined_glb_cat = torch.reshape(combined_glb_cat,\n                                   (combined_glb_cat.shape[0] * combined_glb_cat.shape[1], combined_glb_cat.shape[2])).unsqueeze(1)\n        x_glb = x_glb_ori + combined_glb_cat\n        x_glb = self.norm2(x_glb)\n\n        y = x = torch.cat([x[:, :-1, :], x_glb], dim=1)\n\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n\n        return self.norm3(x + y)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, layers, norm_layer=None, projection=None):\n        super(Encoder, self).__init__()\n        self.layers = nn.ModuleList(layers)\n        self.norm = norm_layer\n        self.projection = projection\n\n    def forward(self, x, b, n, d, x_mask=None, tau=None, delta=None):\n        for layer in self.layers:\n            x = layer(x, b, n, d, x_mask=x_mask, tau=tau, delta=delta)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        if self.projection is not None:\n            x = self.projection(x)\n        return x\n\nclass Model(nn.Module):\n\n    def __init__(self, configs):\n        super(Model, self).__init__()\n        self.seq_len = configs.seq_len\n        self.pred_len = configs.pred_len\n        self.use_norm = configs.use_norm\n        self.patch_len = configs.patch_len\n        self.patch_num = int(configs.seq_len // configs.patch_len)\n        self.kernel_size = configs.kernel_size\n        \n        self.en_embedding = EnEmbedding(configs.n_vars, configs.d_model, self.patch_len, self.kernel_size, configs.dropout)\n\n        self.encoder = Encoder(\n            [\n                EncoderLayer(\n                     AttentionLayer(\n                        FullAttention(False, configs.factor, attention_dropout=configs.dropout,\n                                      output_attention=False),\n                        configs.d_model, configs.n_heads),\n                    configs.d_model,\n                    configs.d_core,\n                    configs.d_ff,\n                    configs.dropout\n                )\n                for l in range(configs.e_layers)\n            ],\n            norm_layer=torch.nn.LayerNorm(configs.d_model)\n        )\n        self.head_nf = configs.d_model * (self.patch_num + 1)\n        self.head = FlattenHead(configs.n_vars, self.head_nf, configs.pred_len,\n                                head_dropout=configs.dropout)\n\n    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n        # Normalization from Non-stationary Transformer\n        if self.use_norm:\n            means = x_enc.mean(1, keepdim=True).detach()\n            x_enc = x_enc - means\n            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n            x_enc /= stdev\n\n        _, _, N = x_enc.shape\n        enc_out,b,n,d = self.en_embedding(x_enc.permute(0,2,1))\n        enc_out = self.encoder(enc_out, b, n, d)\n        enc_out = torch.reshape(\n            enc_out, (-1, n, enc_out.shape[-2], enc_out.shape[-1]))\n        # z: [bs x nvars x d_model x patch_num]\n        enc_out = enc_out.permute(0, 1, 3, 2)\n        \n        dec_out = self.head(enc_out)  # z: [bs x nvars x target_window]\n        dec_out = dec_out.permute(0, 2, 1)\n        \n         # De-Normalization from Non-stationary Transformer\n        if self.use_norm:\n            dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))\n            dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))\n        return dec_out\n    \n    def forward(self, x_enc, x_mark_enc=None, x_dec=None, x_mark_dec=None, mask=None):\n        dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:43:38.732795Z","iopub.execute_input":"2025-10-17T04:43:38.733077Z","iopub.status.idle":"2025-10-17T04:43:38.751590Z","shell.execute_reply.started":"2025-10-17T04:43:38.733060Z","shell.execute_reply":"2025-10-17T04:43:38.750934Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\nsize = [96,48,96]\n\ntrain_set = Dataset_Custom(\n    root_path='//kaggle/input/private-tras/',\n    data_path='traffic.csv',\n    flag='train',\n    size=size,\n    features='M',      # 'M' = multivariate (use all features)\n    target='OT',  # change this to your target column\n    scale=True,\n    timeenc=0,\n    freq='h'           # depends on your dataset frequency (h=hourly, d=daily, etc.)\n)\n\nval_set = Dataset_Custom(\n    root_path='//kaggle/input/private-tras/',\n    data_path='traffic.csv',\n    flag='val',\n    size=size,\n    features='M',\n    target='OT',\n    scale=True,\n    timeenc=0,\n    freq='h'\n)\n\ntest_set = Dataset_Custom(\n    root_path='//kaggle/input/private-tras/',\n    data_path='traffic.csv',\n    flag='test',\n    size=size,\n    features='M',\n    target='OT',\n    scale=True,\n    timeenc=0,\n    freq='h'\n)\n\n\ntrain_loader = DataLoader(train_set, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=16, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=16, shuffle=False)\n\n\n\n\nfor batch_idx, (seq_x, seq_y, seq_x_mark, seq_y_mark) in enumerate(train_loader):\n    print(\"Batch index:\", batch_idx)\n    \n    # Print the first sample in this batch\n    print(\"seq_x shape:\", seq_x.shape)\n    \n    print(\"\\nseq_y shape:\", seq_y.shape)\n    \n    print(\"\\nseq_x_mark shape:\", seq_x_mark.shape)\n    \n    print(\"\\nseq_y_mark shape:\", seq_y_mark.shape)\n    \n    # Exit after first batch\n    break\n\n\n\nclass Config:\n    def __init__(self):\n        self.d_model = 512\n        self.d_core = 128\n        self.e_layers = 2\n        self.d_ff = 512\n        self.n_vars = 862\n        self.seq_len = 96\n        self.pred_len = 96\n        self.kernel_size = 3\n        self.patch_len = 16\n        self.n_heads = 16\n        self.factor = 3\n        self.dropout = 0.1\n        self.use_norm = True\n# Create configuration instance\nconfigs = Config()\nmodel = Model(configs).to('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:43:38.753187Z","iopub.execute_input":"2025-10-17T04:43:38.753413Z","iopub.status.idle":"2025-10-17T04:43:47.894179Z","shell.execute_reply.started":"2025-10-17T04:43:38.753397Z","shell.execute_reply":"2025-10-17T04:43:47.893352Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/3032374269.py:78: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n/tmp/ipykernel_37/3032374269.py:79: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n/tmp/ipykernel_37/3032374269.py:80: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n/tmp/ipykernel_37/3032374269.py:81: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n/tmp/ipykernel_37/3032374269.py:78: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n/tmp/ipykernel_37/3032374269.py:79: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n/tmp/ipykernel_37/3032374269.py:80: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n/tmp/ipykernel_37/3032374269.py:81: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n/tmp/ipykernel_37/3032374269.py:78: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n/tmp/ipykernel_37/3032374269.py:79: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n/tmp/ipykernel_37/3032374269.py:80: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n/tmp/ipykernel_37/3032374269.py:81: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n","output_type":"stream"},{"name":"stdout","text":"Batch index: 0\nseq_x shape: torch.Size([16, 96, 862])\n\nseq_y shape: torch.Size([16, 144, 862])\n\nseq_x_mark shape: torch.Size([16, 96, 4])\n\nseq_y_mark shape: torch.Size([16, 144, 4])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom tqdm import tqdm\n\n# ---- Loss & Metrics ----\nmse_loss = nn.MSELoss()\n\ndef mae_loss(pred, true):\n    return torch.mean(torch.abs(pred - true))\n\n# ---- Train & Evaluate ----\ndef train(model, train_loader, optimizer, device, pred_len):\n    model.train()\n    total_loss = 0\n    for seq_x, seq_y, seq_x_mark, seq_y_mark in tqdm(train_loader, desc=\"Training\", leave=False):\n        seq_x = seq_x.to(device).float()\n        target = seq_y[:, -pred_len:, :].to(device).float()  # take last pred_len steps\n\n        optimizer.zero_grad()\n        outputs = model(seq_x)\n        loss = mse_loss(outputs, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(train_loader)\n\n\ndef evaluate(model, val_loader, device, pred_len):\n    model.eval()\n    total_mse, total_mae = 0, 0\n    with torch.no_grad():\n        for seq_x, seq_y, seq_x_mark, seq_y_mark in tqdm(val_loader, desc=\"Validating\", leave=False):\n            seq_x = seq_x.to(device).float()\n            target = seq_y[:, -pred_len:, :].to(device).float()\n\n            outputs = model(seq_x)\n            total_mse += mse_loss(outputs, target).item()\n            total_mae += mae_loss(outputs, target).item()\n\n    return total_mse / len(val_loader), total_mae / len(val_loader)\n\n\ndef test(model, test_loader, device, pred_len):\n    model.eval()\n    total_mse, total_mae = 0, 0\n    preds, trues = [], []\n    with torch.no_grad():\n        for seq_x, seq_y, seq_x_mark, seq_y_mark in tqdm(test_loader, desc=\"Testing\", leave=False):\n            seq_x = seq_x.to(device).float()\n            target = seq_y[:, -pred_len:, :].to(device).float()\n\n            outputs = model(seq_x)\n            total_mse += mse_loss(outputs, target).item()\n            total_mae += mae_loss(outputs, target).item()\n\n            preds.append(outputs.cpu().numpy())\n            trues.append(target.cpu().numpy())\n\n    preds = np.concatenate(preds, axis=0)\n    trues = np.concatenate(trues, axis=0)\n    return total_mse / len(test_loader), total_mae / len(test_loader), preds, trues\n\n\n# ---- Main Training Loop ----\ndef train_model(model, train_loader, val_loader, test_loader, pred_len, epochs=40, lr=1e-4, patience=5, device='cuda'):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    \n    best_val_loss = float('inf')\n    best_model = None\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        \n        train_loss = train(model, train_loader, optimizer, device, pred_len)\n        val_mse, val_mae = evaluate(model, val_loader, device, pred_len)\n        scheduler.step()\n\n        print(f\"Train Loss: {train_loss:.6f} | Val MSE: {val_mse:.6f} | Val MAE: {val_mae:.6f}\")\n\n        # Early stopping\n        if val_mse < best_val_loss:\n            best_val_loss = val_mse\n            best_model = model.state_dict()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    # Load best model\n    model.load_state_dict(best_model)\n\n    # Final test\n    test_mse, test_mae, preds, trues = test(model, test_loader, device, pred_len)\n    print(f\"\\nTest MSE: {test_mse:.6f} | Test MAE: {test_mae:.6f}\")\n    return model, preds, trues\n\n\n# ---- Run training ----\ntrain_model(model, train_loader, val_loader, test_loader, pred_len=size[2])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:43:47.894963Z","iopub.execute_input":"2025-10-17T04:43:47.895155Z","iopub.status.idle":"2025-10-17T10:22:53.599253Z","shell.execute_reply.started":"2025-10-17T04:43:47.895140Z","shell.execute_reply":"2025-10-17T10:22:53.598621Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.748374 | Val MSE: 0.559115 | Val MAE: 0.397738\n\nEpoch 2/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.456530 | Val MSE: 0.483800 | Val MAE: 0.348942\n\nEpoch 3/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.412743 | Val MSE: 0.455239 | Val MAE: 0.329367\n\nEpoch 4/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.391332 | Val MSE: 0.445809 | Val MAE: 0.319737\n\nEpoch 5/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.376868 | Val MSE: 0.433942 | Val MAE: 0.309634\n\nEpoch 6/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.366068 | Val MSE: 0.427044 | Val MAE: 0.302650\n\nEpoch 7/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.358050 | Val MSE: 0.422404 | Val MAE: 0.297741\n\nEpoch 8/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.352003 | Val MSE: 0.414098 | Val MAE: 0.295026\n\nEpoch 9/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.347195 | Val MSE: 0.408665 | Val MAE: 0.292184\n\nEpoch 10/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.342423 | Val MSE: 0.413857 | Val MAE: 0.286535\n\nEpoch 11/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.338556 | Val MSE: 0.404600 | Val MAE: 0.284475\n\nEpoch 12/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.335034 | Val MSE: 0.400104 | Val MAE: 0.280968\n\nEpoch 13/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.331694 | Val MSE: 0.400620 | Val MAE: 0.276623\n\nEpoch 14/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.329057 | Val MSE: 0.394787 | Val MAE: 0.277181\n\nEpoch 15/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.326692 | Val MSE: 0.392521 | Val MAE: 0.276312\n\nEpoch 16/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.323817 | Val MSE: 0.388921 | Val MAE: 0.272667\n\nEpoch 17/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.321949 | Val MSE: 0.388775 | Val MAE: 0.268970\n\nEpoch 18/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.320397 | Val MSE: 0.383304 | Val MAE: 0.267978\n\nEpoch 19/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.318426 | Val MSE: 0.383465 | Val MAE: 0.267785\n\nEpoch 20/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.316936 | Val MSE: 0.380940 | Val MAE: 0.266613\n\nEpoch 21/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.315552 | Val MSE: 0.380431 | Val MAE: 0.262440\n\nEpoch 22/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.314140 | Val MSE: 0.378818 | Val MAE: 0.262336\n\nEpoch 23/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.312957 | Val MSE: 0.377354 | Val MAE: 0.260821\n\nEpoch 24/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.311808 | Val MSE: 0.376309 | Val MAE: 0.258825\n\nEpoch 25/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.311170 | Val MSE: 0.375875 | Val MAE: 0.257781\n\nEpoch 26/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.310059 | Val MSE: 0.374341 | Val MAE: 0.256648\n\nEpoch 27/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.309092 | Val MSE: 0.375208 | Val MAE: 0.259639\n\nEpoch 28/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.308398 | Val MSE: 0.373454 | Val MAE: 0.257447\n\nEpoch 29/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.307800 | Val MSE: 0.373544 | Val MAE: 0.256268\n\nEpoch 30/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.307092 | Val MSE: 0.373170 | Val MAE: 0.257151\n\nEpoch 31/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.306711 | Val MSE: 0.371873 | Val MAE: 0.256189\n\nEpoch 32/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.306191 | Val MSE: 0.371879 | Val MAE: 0.256713\n\nEpoch 33/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.305805 | Val MSE: 0.371841 | Val MAE: 0.254723\n\nEpoch 34/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.305535 | Val MSE: 0.371002 | Val MAE: 0.254659\n\nEpoch 35/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.305192 | Val MSE: 0.371669 | Val MAE: 0.254978\n\nEpoch 36/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.305069 | Val MSE: 0.371305 | Val MAE: 0.255070\n\nEpoch 37/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.304913 | Val MSE: 0.371135 | Val MAE: 0.254937\n\nEpoch 38/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.304833 | Val MSE: 0.371014 | Val MAE: 0.254338\n\nEpoch 39/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.304651 | Val MSE: 0.371034 | Val MAE: 0.254592\nEarly stopping triggered.\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nTest MSE: 0.437102 | Test MAE: 0.280122\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(Model(\n   (en_embedding): EnEmbedding(\n     (pre_conv): ChannelWiseConv(\n       (conv): Conv1d(862, 862, kernel_size=(3,), stride=(1,), padding=(1,), groups=862)\n     )\n     (value_embedding): Linear(in_features=16, out_features=512, bias=False)\n     (position_embedding): PositionalEmbedding()\n     (dropout): Dropout(p=0.1, inplace=False)\n   )\n   (encoder): Encoder(\n     (layers): ModuleList(\n       (0-1): 2 x EncoderLayer(\n         (self_attention): AttentionLayer(\n           (inner_attention): FullAttention(\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n           (query_projection): Linear(in_features=512, out_features=512, bias=True)\n           (key_projection): Linear(in_features=512, out_features=512, bias=True)\n           (value_projection): Linear(in_features=512, out_features=512, bias=True)\n           (out_projection): Linear(in_features=512, out_features=512, bias=True)\n         )\n         (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n         (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n         (gen1): Linear(in_features=512, out_features=512, bias=True)\n         (gen2): Linear(in_features=512, out_features=128, bias=True)\n         (gen3): Linear(in_features=640, out_features=512, bias=True)\n         (gen4): Linear(in_features=512, out_features=512, bias=True)\n         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n         (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n         (dropout): Dropout(p=0.1, inplace=False)\n       )\n     )\n     (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n   )\n   (head): FlattenHead(\n     (flatten): Flatten(start_dim=-2, end_dim=-1)\n     (linear): Linear(in_features=3584, out_features=96, bias=True)\n     (dropout): Dropout(p=0.1, inplace=False)\n   )\n ),\n array([[[-0.10893436, -0.188084  , -0.36367708, ..., -0.94799674,\n           0.3500923 , -0.16708058],\n         [-0.23070562, -0.2522433 , -0.49399295, ..., -1.1889853 ,\n          -0.21119285, -0.53438324],\n         [-0.3467629 , -0.40725687, -0.68564576, ..., -1.4709117 ,\n          -0.7278868 , -0.9332725 ],\n         ...,\n         [ 0.04713035, -0.26810762,  0.0263128 , ...,  0.41566253,\n           1.7598418 ,  1.6972271 ],\n         [-0.11656927, -0.33836037, -0.3289719 , ..., -0.04548705,\n           1.2548175 ,  1.1059165 ],\n         [-0.23233786, -0.4080921 , -0.56526136, ..., -0.43002424,\n           0.7745036 ,  0.54417914]],\n \n        [[-0.22293574, -0.30442336, -0.58632946, ..., -1.168369  ,\n          -0.17005807, -0.47711992],\n         [-0.3171415 , -0.44421804, -0.73186797, ..., -1.463177  ,\n          -0.64745504, -0.8853009 ],\n         [-0.33848757, -0.5827929 , -0.9191004 , ..., -1.6852858 ,\n          -0.9783569 , -1.212799  ],\n         ...,\n         [-0.08917863, -0.2900241 , -0.2682011 , ...,  0.08260226,\n           1.2780871 ,  1.2016549 ],\n         [-0.19653408, -0.33437636, -0.5185101 , ..., -0.36748767,\n           0.8204634 ,  0.59728265],\n         [-0.30603263, -0.43662927, -0.65790653, ..., -0.670339  ,\n           0.3422287 ,  0.13831306]],\n \n        [[-0.44305086, -0.54284954, -0.841595  , ..., -1.4510388 ,\n          -0.71941435, -0.89565843],\n         [-0.49444324, -0.6999809 , -0.97920614, ..., -1.6877851 ,\n          -1.0086145 , -1.2226758 ],\n         [-0.5097641 , -0.7614354 , -1.0633457 , ..., -1.8186281 ,\n          -1.1547492 , -1.373126  ],\n         ...,\n         [-0.16798407, -0.29907554, -0.467951  , ..., -0.32385108,\n           0.78859687,  0.6487179 ],\n         [-0.25780565, -0.37022293, -0.57175046, ..., -0.6488325 ,\n           0.32105148,  0.16201465],\n         [-0.41166145, -0.534837  , -0.67354333, ..., -0.9768148 ,\n          -0.23325539, -0.30653638]],\n \n        ...,\n \n        [[-0.7687858 , -0.7027957 , -0.8628782 , ..., -0.31663108,\n          -0.5880834 , -0.5231093 ],\n         [-0.93730265, -0.8103568 , -0.98515606, ..., -0.5567094 ,\n          -1.0410578 , -0.95795405],\n         [-1.014331  , -0.9058923 , -1.1333039 , ..., -0.7447925 ,\n          -1.3619971 , -1.2620054 ],\n         ...,\n         [-0.28068888, -0.40864536, -0.42992625, ...,  0.08772679,\n           0.7129845 ,  0.42323068],\n         [-0.32662585, -0.4241334 , -0.53096235, ..., -0.06847033,\n           0.4166811 ,  0.0840927 ],\n         [-0.40309313, -0.48285857, -0.6187782 , ..., -0.2087946 ,\n           0.16499183, -0.1777673 ]],\n \n        [[-0.90299976, -0.82641864, -0.9914661 , ..., -0.58097154,\n          -0.9740789 , -0.98271453],\n         [-0.99163115, -0.8654577 , -1.1057671 , ..., -0.78925544,\n          -1.3013436 , -1.3157407 ],\n         [-1.0184778 , -0.8675201 , -1.1830244 , ..., -0.89079183,\n          -1.4967537 , -1.4585353 ],\n         ...,\n         [-0.3260229 , -0.43791166, -0.52134943, ...,  0.01639806,\n           0.5743749 ,  0.22752516],\n         [-0.40004152, -0.45195758, -0.57723695, ..., -0.13222495,\n           0.20808667, -0.09362686],\n         [-0.5161107 , -0.5334428 , -0.6840227 , ..., -0.32001966,\n          -0.16296685, -0.42658713]],\n \n        [[-0.92711896, -0.8769405 , -1.0754435 , ..., -0.7772498 ,\n          -1.2054019 , -1.353507  ],\n         [-0.97398657, -0.8789767 , -1.1534468 , ..., -0.91873145,\n          -1.3925813 , -1.5417022 ],\n         [-0.9622305 , -0.864155  , -1.1798493 , ..., -0.95490515,\n          -1.3481812 , -1.5193136 ],\n         ...,\n         [-0.35421342, -0.45518368, -0.56077045, ..., -0.08005864,\n           0.28689426, -0.0021209 ],\n         [-0.47429115, -0.50230575, -0.6642966 , ..., -0.2663218 ,\n          -0.17090678, -0.38516638],\n         [-0.5962479 , -0.5926174 , -0.8005447 , ..., -0.49649236,\n          -0.60525817, -0.7758123 ]]], dtype=float32),\n array([[[-0.14757378, -0.16522172, -0.1978649 , ..., -0.90306795,\n           0.6016803 , -0.18149094],\n         [-0.48413023, -0.39308676, -0.6169031 , ..., -1.1438346 ,\n          -0.344122  , -0.58670765],\n         [-0.71122414, -0.60060674, -0.87910837, ..., -1.3889005 ,\n          -0.88647014, -0.8949006 ],\n         ...,\n         [ 0.03704212, -0.29746482,  1.174424  , ...,  0.70060945,\n           1.3424485 ,  1.9530307 ],\n         [-0.07405417, -0.29746482, -0.20889223, ...,  0.18468107,\n           1.3490624 ,  1.2738647 ],\n         [-0.15247509, -0.33001697, -0.5335856 , ..., -0.23666045,\n           0.6545923 ,  0.72025883]],\n \n        [[-0.48413023, -0.39308676, -0.6169031 , ..., -1.1438346 ,\n          -0.344122  , -0.58670765],\n         [-0.71122414, -0.60060674, -0.87910837, ..., -1.3889005 ,\n          -0.88647014, -0.8949006 ],\n         [-0.6066629 , -0.7267463 , -1.0432929 , ..., -1.6425654 ,\n          -1.0055221 , -1.1973863 ],\n         ...,\n         [-0.07405417, -0.29746482, -0.20889223, ...,  0.18468107,\n           1.3490624 ,  1.2738647 ],\n         [-0.15247509, -0.33001697, -0.5335856 , ..., -0.23666045,\n           0.6545923 ,  0.72025883],\n         [-0.2782753 , -0.36867267, -0.5997495 , ..., -0.5548163 ,\n           0.41648823,  0.18948206]],\n \n        [[-0.71122414, -0.60060674, -0.87910837, ..., -1.3889005 ,\n          -0.88647014, -0.8949006 ],\n         [-0.6066629 , -0.7267463 , -1.0432929 , ..., -1.6425654 ,\n          -1.0055221 , -1.1973863 ],\n         [-0.5445797 , -0.7898161 , -1.1204842 , ..., -1.7844456 ,\n          -1.1510302 , -1.3914337 ],\n         ...,\n         [-0.15247509, -0.33001697, -0.5335856 , ..., -0.23666045,\n           0.6545923 ,  0.72025883],\n         [-0.2782753 , -0.36867267, -0.5997495 , ..., -0.5548163 ,\n           0.41648823,  0.18948206],\n         [-0.3632313 , -0.5070193 , -0.73942894, ..., -1.0062536 ,\n          -0.45656002, -0.43261114]],\n \n        ...,\n \n        [[-0.6409721 , -0.58636516, -0.8288728 , ..., -0.39143896,\n          -0.6880501 , -0.63807315],\n         [-0.69978774, -0.6880906 , -0.99305737, ..., -0.62790614,\n          -1.0452062 , -0.95768064],\n         [-0.71122414, -0.77760905, -1.0739244 , ..., -0.7310918 ,\n          -1.3031522 , -1.16885   ],\n         ...,\n         [-0.23089601, -0.26491266,  0.40863782, ..., -1.1395352 ,\n           0.5752243 ,  0.76591706],\n         [-0.32402086, -0.4256389 , -0.6156779 , ..., -1.3029125 ,\n           0.3437342 ,  0.38923678],\n         [-0.33872476, -0.41750088, -0.60465056, ..., -1.3502059 ,\n           0.25113818,  0.17236024]],\n \n        [[-0.69978774, -0.6880906 , -0.99305737, ..., -0.62790614,\n          -1.0452062 , -0.95768064],\n         [-0.71122414, -0.77760905, -1.0739244 , ..., -0.7310918 ,\n          -1.3031522 , -1.16885   ],\n         [-0.7161254 , -0.8264373 , -1.1192589 , ..., -0.88587034,\n          -1.3031522 , -1.3800192 ],\n         ...,\n         [-0.32402086, -0.4256389 , -0.6156779 , ..., -1.3029125 ,\n           0.3437342 ,  0.38923678],\n         [-0.33872476, -0.41750088, -0.60465056, ..., -1.3502059 ,\n           0.25113818,  0.17236024],\n         [-0.41061062, -0.47446716, -0.67203975, ..., -1.4447927 ,\n          -0.0134219 , -0.09017452]],\n \n        [[-0.71122414, -0.77760905, -1.0739244 , ..., -0.7310918 ,\n          -1.3031522 , -1.16885   ],\n         [-0.7161254 , -0.8264373 , -1.1192589 , ..., -0.88587034,\n          -1.3031522 , -1.3800192 ],\n         [-0.7210267 , -0.82236826, -1.1094569 , ..., -0.90306795,\n          -1.3560643 , -1.4142628 ],\n         ...,\n         [-0.33872476, -0.41750088, -0.60465056, ..., -1.3502059 ,\n           0.25113818,  0.17236024],\n         [-0.41061062, -0.47446716, -0.67203975, ..., -1.4447927 ,\n          -0.0134219 , -0.09017452],\n         [-0.64260584, -0.5985722 , -0.83254856, ..., -1.5823736 ,\n          -0.55577004, -0.4953912 ]]], dtype=float32))"},"metadata":{}}],"execution_count":7}]}