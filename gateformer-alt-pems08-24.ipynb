{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13225443,"sourceType":"datasetVersion","datasetId":8383027},{"sourceId":13229637,"sourceType":"datasetVersion","datasetId":8385826}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# From: gluonts/src/gluonts/time_feature/_base.py\n# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\").\n# You may not use this file except in compliance with the License.\n# A copy of the License is located at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# or in the \"license\" file accompanying this file. This file is distributed\n# on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n# express or implied. See the License for the specific language governing\n# permissions and limitations under the License.\n\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\n\n\nclass TimeFeature:\n    def __init__(self):\n        pass\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        pass\n\n    def __repr__(self):\n        return self.__class__.__name__ + \"()\"\n\n\nclass SecondOfMinute(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.second / 59.0 - 0.5\n\n\nclass MinuteOfHour(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.minute / 59.0 - 0.5\n\n\nclass HourOfDay(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.hour / 23.0 - 0.5\n\n\nclass DayOfWeek(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.dayofweek / 6.0 - 0.5\n\n\nclass DayOfMonth(TimeFeature):\n    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.day - 1) / 30.0 - 0.5\n\n\nclass DayOfYear(TimeFeature):\n    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.dayofyear - 1) / 365.0 - 0.5\n\n\nclass MonthOfYear(TimeFeature):\n    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.month - 1) / 11.0 - 0.5\n\n\nclass WeekOfYear(TimeFeature):\n    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.isocalendar().week - 1) / 52.0 - 0.5\n\n\ndef time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n    \"\"\"\n    Returns a list of time features that will be appropriate for the given frequency string.\n    Parameters\n    ----------\n    freq_str\n        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n    \"\"\"\n\n    features_by_offsets = {\n        offsets.YearEnd: [],\n        offsets.QuarterEnd: [MonthOfYear],\n        offsets.MonthEnd: [MonthOfYear],\n        offsets.Week: [DayOfMonth, WeekOfYear],\n        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Minute: [\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n        offsets.Second: [\n            SecondOfMinute,\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n    }\n\n    offset = to_offset(freq_str)\n\n    for offset_type, feature_classes in features_by_offsets.items():\n        if isinstance(offset, offset_type):\n            return [cls() for cls in feature_classes]\n\n    supported_freq_msg = f\"\"\"\n    Unsupported frequency {freq_str}\n    The following frequencies are supported:\n        Y   - yearly\n            alias: A\n        M   - monthly\n        W   - weekly\n        D   - daily\n        B   - business days\n        H   - hourly\n        T   - minutely\n            alias: min\n        S   - secondly\n    \"\"\"\n    raise RuntimeError(supported_freq_msg)\n\n\ndef time_features(dates, freq='h'):\n    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:57:03.671553Z","iopub.execute_input":"2025-10-02T20:57:03.671849Z","iopub.status.idle":"2025-10-02T20:57:03.685843Z","shell.execute_reply.started":"2025-10-02T20:57:03.671828Z","shell.execute_reply":"2025-10-02T20:57:03.685230Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n\n\nsize = [96,48,24]\n\n\nclass Dataset_Custom(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTh1.csv',\n                 target='OT', scale=True, timeenc=0, freq='h'):\n        # size [seq_len, label_len, pred_len]\n        # info\n        if size == None:\n            self.seq_len = 24 * 4 * 4\n            self.label_len = 24 * 4\n            self.pred_len = 24 * 4\n        else:\n            self.seq_len = size[0]\n            self.label_len = size[1]\n            self.pred_len = size[2]\n        # init\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n\n        self.features = features\n        self.target = target\n        self.scale = scale\n        self.timeenc = timeenc\n        self.freq = freq\n\n        self.root_path = root_path\n        self.data_path = data_path\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        df_raw = pd.read_csv(os.path.join(self.root_path,\n                                          self.data_path))\n\n        '''\n        df_raw.columns: ['date', ...(other features), target feature]\n        '''\n        cols = list(df_raw.columns)\n        cols.remove(self.target)\n        cols.remove('date')\n        df_raw = df_raw[['date'] + cols + [self.target]]\n        num_train = int(len(df_raw) * 0.7)\n        num_test = int(len(df_raw) * 0.2)\n        num_vali = len(df_raw) - num_train - num_test\n        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n        border2s = [num_train, num_train + num_vali, len(df_raw)]\n        border1 = border1s[self.set_type]\n        border2 = border2s[self.set_type]\n\n        if self.features == 'M' or self.features == 'MS':\n            cols_data = df_raw.columns[1:]\n            df_data = df_raw[cols_data]\n        elif self.features == 'S':\n            df_data = df_raw[[self.target]]\n\n        if self.scale:\n            train_data = df_data[border1s[0]:border2s[0]]\n            self.scaler.fit(train_data.values)\n            data = self.scaler.transform(df_data.values)\n        else:\n            data = df_data.values\n\n        df_stamp = df_raw[['date']][border1:border2]\n        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n        if self.timeenc == 0:\n            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n            data_stamp = df_stamp.drop(['date'], axis=1).values\n        elif self.timeenc == 1:\n            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n            data_stamp = data_stamp.transpose(1, 0)\n\n        self.data_x = data[border1:border2]\n        self.data_y = data[border1:border2]\n        self.data_stamp = data_stamp\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return len(self.data_x) - self.seq_len - self.pred_len + 1\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\nclass Dataset_Solar(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTh1.csv',\n                 target='OT', scale=True, timeenc=0, freq='h'):\n        # size [seq_len, label_len, pred_len]\n        # info\n        self.seq_len = size[0]\n        self.label_len = size[1]\n        self.pred_len = size[2]\n        # init\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n\n        self.features = features\n        self.target = target\n        self.scale = scale\n        self.timeenc = timeenc\n        self.freq = freq\n\n        self.root_path = root_path\n        self.data_path = data_path\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        df_raw = []\n        with open(os.path.join(self.root_path, self.data_path), \"r\", encoding='utf-8') as f:\n            for line in f.readlines():\n                line = line.strip('\\n').split(',')\n                data_line = np.stack([float(i) for i in line[1:]])\n                df_raw.append(data_line)\n        df_raw = np.stack(df_raw, 0)\n        df_raw = pd.DataFrame(df_raw)\n\n        num_train = int(len(df_raw) * 0.7)\n        num_test = int(len(df_raw) * 0.2)\n        num_valid = int(len(df_raw) * 0.1)\n        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n        border2s = [num_train, num_train + num_valid, len(df_raw)]\n        border1 = border1s[self.set_type]\n        border2 = border2s[self.set_type]\n\n        df_data = df_raw.values\n\n        if self.scale:\n            train_data = df_data[border1s[0]:border2s[0]]\n            self.scaler.fit(train_data)\n            data = self.scaler.transform(df_data)\n        else:\n            data = df_data\n\n        self.data_x = data[border1:border2]\n        self.data_y = data[border1:border2]\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n        seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return len(self.data_x) - self.seq_len - self.pred_len + 1\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\n\nclass Dataset_ETT_hour(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTh1.csv',\n                 target='OT', scale=True, timeenc=0, freq='h'):\n        # size [seq_len, label_len, pred_len]\n        # info\n        if size == None:\n            self.seq_len = 24 * 4 * 4\n            self.label_len = 24 * 4\n            self.pred_len = 24 * 4\n        else:\n            self.seq_len = size[0]\n            self.label_len = size[1]\n            self.pred_len = size[2]\n        # init\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n\n        self.features = features\n        self.target = target\n        self.scale = scale\n        self.timeenc = timeenc\n        self.freq = freq\n\n        self.root_path = root_path\n        self.data_path = data_path\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        df_raw = pd.read_csv(os.path.join(self.root_path,\n                                          self.data_path))\n\n        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n        border1 = border1s[self.set_type]\n        border2 = border2s[self.set_type]\n\n        if self.features == 'M' or self.features == 'MS':\n            cols_data = df_raw.columns[1:]\n            df_data = df_raw[cols_data]\n        elif self.features == 'S':\n            df_data = df_raw[[self.target]]\n\n        if self.scale:\n            train_data = df_data[border1s[0]:border2s[0]]\n            self.scaler.fit(train_data.values)\n            data = self.scaler.transform(df_data.values)\n        else:\n            data = df_data.values\n\n        df_stamp = df_raw[['date']][border1:border2]\n        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n        if self.timeenc == 0:\n            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n            data_stamp = df_stamp.drop(['date'], axis=1).values\n        elif self.timeenc == 1:\n            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n            data_stamp = data_stamp.transpose(1, 0)\n\n        self.data_x = data[border1:border2]\n        self.data_y = data[border1:border2]\n        self.data_stamp = data_stamp\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return len(self.data_x) - self.seq_len - self.pred_len + 1\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\nclass Dataset_ETT_minute(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTm1.csv',\n                 target='OT', scale=True, timeenc=0, freq='t'):\n        # size [seq_len, label_len, pred_len]\n        # info\n        if size == None:\n            self.seq_len = 24 * 4 * 4\n            self.label_len = 24 * 4\n            self.pred_len = 24 * 4\n        else:\n            self.seq_len = size[0]\n            self.label_len = size[1]\n            self.pred_len = size[2]\n        # init\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n\n        self.features = features\n        self.target = target\n        self.scale = scale\n        self.timeenc = timeenc\n        self.freq = freq\n\n        self.root_path = root_path\n        self.data_path = data_path\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        df_raw = pd.read_csv(os.path.join(self.root_path,\n                                          self.data_path))\n\n        border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]\n        border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n        border1 = border1s[self.set_type]\n        border2 = border2s[self.set_type]\n\n        if self.features == 'M' or self.features == 'MS':\n            cols_data = df_raw.columns[1:]\n            df_data = df_raw[cols_data]\n        elif self.features == 'S':\n            df_data = df_raw[[self.target]]\n\n        if self.scale:\n            train_data = df_data[border1s[0]:border2s[0]]\n            self.scaler.fit(train_data.values)\n            data = self.scaler.transform(df_data.values)\n        else:\n            data = df_data.values\n\n        df_stamp = df_raw[['date']][border1:border2]\n        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n        if self.timeenc == 0:\n            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n            data_stamp = df_stamp.drop(['date'], axis=1).values\n        elif self.timeenc == 1:\n            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n            data_stamp = data_stamp.transpose(1, 0)\n\n        self.data_x = data[border1:border2]\n        self.data_y = data[border1:border2]\n        self.data_stamp = data_stamp\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return len(self.data_x) - self.seq_len - self.pred_len + 1\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\n\nclass Dataset_PEMS(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTh1.csv',\n                 target='OT', scale=True, timeenc=0, freq='h'):\n        # size [seq_len, label_len, pred_len]\n        # info\n        self.seq_len = size[0]\n        self.label_len = size[1]\n        self.pred_len = size[2]\n        # init\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n\n        self.features = features\n        self.target = target\n        self.scale = scale\n        self.timeenc = timeenc\n        self.freq = freq\n\n        self.root_path = root_path\n        self.data_path = data_path\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        data_file = os.path.join(self.root_path, self.data_path)\n        data = np.load(data_file, allow_pickle=True)\n        data = data['data'][:, :, 0]\n\n        train_ratio = 0.6\n        valid_ratio = 0.2\n        train_data = data[:int(train_ratio * len(data))]\n        valid_data = data[int(train_ratio * len(data)): int((train_ratio + valid_ratio) * len(data))]\n        test_data = data[int((train_ratio + valid_ratio) * len(data)):]\n        total_data = [train_data, valid_data, test_data]\n        data = total_data[self.set_type]\n\n        if self.scale:\n            self.scaler.fit(train_data)\n            data = self.scaler.transform(data)\n\n        df = pd.DataFrame(data)\n        df = df.fillna(method='ffill', limit=len(df)).fillna(method='bfill', limit=len(df)).values\n\n        self.data_x = df\n        self.data_y = df\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n        seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return len(self.data_x) - self.seq_len - self.pred_len + 1\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\ntrain_set = Dataset_PEMS(\n    root_path='//kaggle/input/private-pmes/',\n    data_path='PEMS08.npz',\n    flag='train',\n    size=size,\n    features='M',      # 'M' = multivariate (use all features)\n    target='OT',  # change this to your target column\n    scale=True,\n    timeenc=0,\n    freq='t'           # depends on your dataset frequency (h=hourly, d=daily, etc.)\n)\n\nval_set = Dataset_PEMS(\n    root_path='//kaggle/input/private-pmes/',\n    data_path='PEMS08.npz',\n    flag='val',\n    size=size,\n    features='M',\n    target='OT',\n    scale=True,\n    timeenc=0,\n    freq='t'\n)\n\ntest_set = Dataset_PEMS(\n    root_path='//kaggle/input/private-pmes/',\n    data_path='PEMS08.npz',\n    flag='test',\n    size=size,\n    features='M',\n    target='OT',\n    scale=True,\n    timeenc=0,\n    freq='t'\n)\n\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n\n\n\n\nfor batch_idx, (seq_x, seq_y, seq_x_mark, seq_y_mark) in enumerate(train_loader):\n    print(\"Batch index:\", batch_idx)\n    \n    # Print the first sample in this batch\n    print(\"seq_x shape:\", seq_x.shape)\n    \n    print(\"\\nseq_y shape:\", seq_y.shape)\n    \n    print(\"\\nseq_x_mark shape:\", seq_x_mark.shape)\n    \n    print(\"\\nseq_y_mark shape:\", seq_y_mark.shape)\n    \n    # Exit after first batch\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:57:03.790196Z","iopub.execute_input":"2025-10-02T20:57:03.790545Z","iopub.status.idle":"2025-10-02T20:57:04.935597Z","shell.execute_reply.started":"2025-10-02T20:57:03.790523Z","shell.execute_reply":"2025-10-02T20:57:04.934782Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3149584877.py:411: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  df = df.fillna(method='ffill', limit=len(df)).fillna(method='bfill', limit=len(df)).values\n/tmp/ipykernel_36/3149584877.py:411: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  df = df.fillna(method='ffill', limit=len(df)).fillna(method='bfill', limit=len(df)).values\n","output_type":"stream"},{"name":"stdout","text":"Batch index: 0\nseq_x shape: torch.Size([32, 96, 170])\n\nseq_y shape: torch.Size([32, 72, 170])\n\nseq_x_mark shape: torch.Size([32, 96, 1])\n\nseq_y_mark shape: torch.Size([32, 96, 1])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3149584877.py:411: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  df = df.fillna(method='ffill', limit=len(df)).fillna(method='bfill', limit=len(df)).values\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"__all__ = ['Transpose', 'get_activation_fn', 'moving_avg', 'series_decomp', 'PositionalEncoding', 'SinCosPosEncoding', 'Coord2dPosEncoding', 'Coord1dPosEncoding', 'positional_encoding']           \n\nimport torch\nfrom torch import nn\nimport math\n\nclass Transpose(nn.Module):\n    def __init__(self, *dims, contiguous=False): \n        super().__init__()\n        self.dims, self.contiguous = dims, contiguous\n    def forward(self, x):\n        if self.contiguous: return x.transpose(*self.dims).contiguous()\n        else: return x.transpose(*self.dims)\n\n    \ndef get_activation_fn(activation):\n    if callable(activation): return activation()\n    elif activation.lower() == \"relu\": return nn.ReLU()\n    elif activation.lower() == \"gelu\": return nn.GELU()\n    raise ValueError(f'{activation} is not available. You can use \"relu\", \"gelu\", or a callable') \n    \n    \n# decomposition\n\nclass moving_avg(nn.Module):\n    \"\"\"\n    Moving average block to highlight the trend of time series\n    \"\"\"\n    def __init__(self, kernel_size, stride):\n        super(moving_avg, self).__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x):\n        # padding on the both ends of time series\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        x = x.permute(0, 2, 1)\n        return x\n\n\nclass series_decomp(nn.Module):\n    \"\"\"\n    Series decomposition block\n    \"\"\"\n    def __init__(self, kernel_size):\n        super(series_decomp, self).__init__()\n        self.moving_avg = moving_avg(kernel_size, stride=1)\n\n    def forward(self, x):\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n    \n    \n    \n# pos_encoding\n\ndef PositionalEncoding(q_len, d_model, normalize=True):\n    pe = torch.zeros(q_len, d_model)\n    position = torch.arange(0, q_len).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    if normalize:\n        pe = pe - pe.mean()\n        pe = pe / (pe.std() * 10)\n    return pe\n\nSinCosPosEncoding = PositionalEncoding\n\ndef Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True, eps=1e-3, verbose=False):\n    x = .5 if exponential else 1\n    i = 0\n    for i in range(100):\n        cpe = 2 * (torch.linspace(0, 1, q_len).reshape(-1, 1) ** x) * (torch.linspace(0, 1, d_model).reshape(1, -1) ** x) - 1\n        pv(f'{i:4.0f}  {x:5.3f}  {cpe.mean():+6.3f}', verbose)\n        if abs(cpe.mean()) <= eps: break\n        elif cpe.mean() > eps: x += .001\n        else: x -= .001\n        i += 1\n    if normalize:\n        cpe = cpe - cpe.mean()\n        cpe = cpe / (cpe.std() * 10)\n    return cpe\n\ndef Coord1dPosEncoding(q_len, exponential=False, normalize=True):\n    cpe = (2 * (torch.linspace(0, 1, q_len).reshape(-1, 1)**(.5 if exponential else 1)) - 1)\n    if normalize:\n        cpe = cpe - cpe.mean()\n        cpe = cpe / (cpe.std() * 10)\n    return cpe\n\ndef positional_encoding(pe, learn_pe, q_len, d_model):\n    # Positional encoding\n    if pe == None:\n        W_pos = torch.empty((q_len, d_model)) # pe = None and learn_pe = False can be used to measure impact of pe\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n        learn_pe = False\n    elif pe == 'zero':\n        W_pos = torch.empty((q_len, 1))\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n    elif pe == 'zeros':\n        W_pos = torch.empty((q_len, d_model))\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n    elif pe == 'normal' or pe == 'gauss':\n        W_pos = torch.zeros((q_len, 1))\n        torch.nn.init.normal_(W_pos, mean=0.0, std=0.1)\n    elif pe == 'uniform':\n        W_pos = torch.zeros((q_len, 1))\n        nn.init.uniform_(W_pos, a=0.0, b=0.1)\n    elif pe == 'lin1d': W_pos = Coord1dPosEncoding(q_len, exponential=False, normalize=True)\n    elif pe == 'exp1d': W_pos = Coord1dPosEncoding(q_len, exponential=True, normalize=True)\n    elif pe == 'lin2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True)\n    elif pe == 'exp2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=True, normalize=True)\n    elif pe == 'sincos': W_pos = PositionalEncoding(q_len, d_model, normalize=True)\n    else: raise ValueError(f\"{pe} is not a valid pe (positional encoder. Available types: 'gauss'=='normal', \\\n        'zeros', 'zero', uniform', 'lin1d', 'exp1d', 'lin2d', 'exp2d', 'sincos', None.)\")\n    return nn.Parameter(W_pos, requires_grad=learn_pe)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:57:04.936859Z","iopub.execute_input":"2025-10-02T20:57:04.937072Z","iopub.status.idle":"2025-10-02T20:57:04.953247Z","shell.execute_reply.started":"2025-10-02T20:57:04.937055Z","shell.execute_reply":"2025-10-02T20:57:04.952575Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass RevIN(nn.Module):\n    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n        \"\"\"\n        :param num_features: the number of features or channels\n        :param eps: a value added for numerical stability\n        :param affine: if True, RevIN has learnable affine parameters\n        \"\"\"\n        super(RevIN, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.subtract_last = subtract_last\n        if self.affine:\n            self._init_params()\n\n    def forward(self, x, mode:str):\n        if mode == 'norm':\n            self._get_statistics(x)\n            x = self._normalize(x)\n        elif mode == 'denorm':\n            x = self._denormalize(x)\n        else: raise NotImplementedError\n        return x\n\n    def _init_params(self):\n        # initialize RevIN params: (C,)\n        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n\n    def _get_statistics(self, x):\n        dim2reduce = tuple(range(1, x.ndim-1))\n        if self.subtract_last:\n            self.last = x[:,-1,:].unsqueeze(1)\n        else:\n            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n\n    def _normalize(self, x):\n        if self.subtract_last:\n            x = x - self.last\n        else:\n            x = x - self.mean\n        x = x / self.stdev\n        if self.affine:\n            x = x * self.affine_weight\n            x = x + self.affine_bias\n        return x\n\n    def _denormalize(self, x):\n        if self.affine:\n            x = x - self.affine_bias\n            x = x / (self.affine_weight + self.eps*self.eps)\n        x = x * self.stdev\n        if self.subtract_last:\n            x = x + self.last\n        else:\n            x = x + self.mean\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:57:04.953937Z","iopub.execute_input":"2025-10-02T20:57:04.954234Z","iopub.status.idle":"2025-10-02T20:57:04.973893Z","shell.execute_reply.started":"2025-10-02T20:57:04.954217Z","shell.execute_reply":"2025-10-02T20:57:04.973150Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\n\n\n__all__ = ['PatchTST_backbone']\n\n# Cell\nfrom typing import Callable, Optional\nimport torch\nfrom torch import nn\nfrom torch import Tensor\nimport torch.nn.functional as F\nimport numpy as np\n\nclass ChannelWiseConv(nn.Module):\n    def __init__(self,c_in,kernel_window, stride=1,padding=3,bias=True):\n        super(ChannelWiseConv,self).__init__()\n        padding = (kernel_window - 1) // 2 if kernel_window % 2 == 1 else kernel_window // 2\n        self.conv = nn.Conv1d(\n            in_channels=c_in, \n            out_channels=c_in, \n            kernel_size=kernel_window, \n            stride=stride, \n            padding=padding, \n            groups=c_in, \n            bias=bias\n        )\n\n    def forward(self, x):\n        # x shape: (batch_size, channels, sequence_length)\n        return self.conv(x)\n\n# Cell\nclass PatchTST_backbone(nn.Module):\n    def __init__(self, c_in:int, context_window:int, target_window:int, patch_len:int, stride:int, kernel_window:int, max_seq_len:Optional[int]=1024, \n                 n_layers:int=3, d_model=128, n_heads=16, d_k:Optional[int]=None, d_v:Optional[int]=None,\n                 d_ff:int=256, norm:str='BatchNorm', attn_dropout:float=0., dropout:float=0., act:str=\"gelu\", key_padding_mask:bool='auto',\n                 padding_var:Optional[int]=None, attn_mask:Optional[Tensor]=None, res_attention:bool=True, pre_norm:bool=False, store_attn:bool=False,\n                 pe:str='zeros', learn_pe:bool=True, fc_dropout:float=0., head_dropout = 0, padding_patch = None,\n                 pretrain_head:bool=False, head_type = 'flatten', individual = False, revin = True, affine = True, subtract_last = False,\n                 verbose:bool=False, **kwargs):\n        \n        super().__init__()\n        \n        # RevIn\n        self.revin = revin\n        if self.revin: self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n        self.channel_wise_conv = ChannelWiseConv(c_in,kernel_window)\n        self.post_conv_norm = nn.BatchNorm1d(c_in)\n        # Patching\n        self.patch_len = patch_len\n        self.stride = stride\n        self.padding_patch = padding_patch\n        patch_num = int((context_window - patch_len)/stride + 1)\n        if padding_patch == 'end': # can be modified to general case\n            self.padding_patch_layer = nn.ReplicationPad1d((0, stride)) \n            patch_num += 1\n        \n        # Backbone \n        self.backbone = TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len, max_seq_len=max_seq_len,\n                                n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n                                attn_dropout=attn_dropout, dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var,\n                                attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n                                pe=pe, learn_pe=learn_pe, verbose=verbose, **kwargs)\n\n        self.temporal_context_linear = nn.Linear(d_model * patch_num, d_model) \n\n        \n    \n    def forward(self, z):                                                                   # z: [bs x nvars x seq_len]\n        # norm\n        if self.revin: \n            z = z.permute(0,2,1)\n            z = self.revin_layer(z, 'norm')\n            z = z.permute(0,2,1)\n\n        \n        \n        #conv1d\n        z = self.channel_wise_conv(z)\n        z = self.post_conv_norm(z) \n        \n        # do patching\n        if self.padding_patch == 'end':\n            z = self.padding_patch_layer(z)\n        z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)                   # z: [bs x nvars x patch_num x patch_len]\n        z = z.permute(0,1,3,2)                                                              # z: [bs x nvars x patch_len x patch_num]\n        \n        # model\n        z = self.backbone(z)                                                                # z: [bs x nvars x d_model x patch_num]\n        \n        B, C, D, P = z.shape\n        z_flat = z.reshape(B, C, D * P) \n        context_vec = self.temporal_context_linear(z_flat)\n           \n        return context_vec\n    \n    def create_pretrain_head(self, head_nf, vars, dropout):\n        return nn.Sequential(nn.Dropout(dropout),\n                    nn.Conv1d(head_nf, vars, 1)\n                    )\n\n\nclass Flatten_Head(nn.Module):\n    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n        super().__init__()\n        \n        self.individual = individual\n        self.n_vars = n_vars\n        \n        if self.individual:\n            self.linears = nn.ModuleList()\n            self.dropouts = nn.ModuleList()\n            self.flattens = nn.ModuleList()\n            for i in range(self.n_vars):\n                self.flattens.append(nn.Flatten(start_dim=-2))\n                self.linears.append(nn.Linear(nf, target_window))\n                self.dropouts.append(nn.Dropout(head_dropout))\n        else:\n            self.flatten = nn.Flatten(start_dim=-2)\n            self.linear = nn.Linear(nf, target_window)\n            self.dropout = nn.Dropout(head_dropout)\n            \n    def forward(self, x):                                 # x: [bs x nvars x d_model x patch_num]\n        if self.individual:\n            x_out = []\n            for i in range(self.n_vars):\n                z = self.flattens[i](x[:,i,:,:])          # z: [bs x d_model * patch_num]\n                z = self.linears[i](z)                    # z: [bs x target_window]\n                z = self.dropouts[i](z)\n                x_out.append(z)\n            x = torch.stack(x_out, dim=1)                 # x: [bs x nvars x target_window]\n        else:\n            x = self.flatten(x)\n            x = self.linear(x)\n            x = self.dropout(x)\n        return x\n        \n        \n    \n    \nclass TSTiEncoder(nn.Module):  #i means channel-independent\n    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., act=\"gelu\", store_attn=False,\n                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n                 pe='zeros', learn_pe=True, verbose=False, **kwargs):\n        \n        \n        super().__init__()\n        \n        self.patch_num = patch_num\n        self.patch_len = patch_len\n        \n        # Input encoding\n        q_len = patch_num\n        self.W_P = nn.Linear(patch_len, d_model)        # Eq 1: projection of feature vectors onto a d-dim vector space\n        self.seq_len = q_len\n\n        # Positional encoding\n        self.W_pos = positional_encoding(pe, learn_pe, q_len, d_model)\n\n        # Residual dropout\n        self.dropout = nn.Dropout(dropout)\n\n        # Encoder\n        self.encoder = TSTEncoder(q_len, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n                                   pre_norm=pre_norm, activation=act, res_attention=res_attention, n_layers=n_layers, store_attn=store_attn)\n\n        \n    def forward(self, x) -> Tensor:                                              # x: [bs x nvars x patch_len x patch_num]\n        \n        n_vars = x.shape[1]\n        # Input encoding\n        x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n        x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n\n        u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]\n        u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]\n\n        # Encoder\n        z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n        z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x nvars x patch_num x d_model]\n        z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]\n        \n        return z    \n            \n            \n    \n# Cell\nclass TSTEncoder(nn.Module):\n    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None, \n                        norm='BatchNorm', attn_dropout=0., dropout=0., activation='gelu',\n                        res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n        super().__init__()\n\n        self.layers = nn.ModuleList([TSTEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n                                                      attn_dropout=attn_dropout, dropout=dropout,\n                                                      activation=activation, res_attention=res_attention,\n                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n        self.res_attention = res_attention\n\n    def forward(self, src:Tensor, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n        output = src\n        scores = None\n        if self.res_attention:\n            for mod in self.layers: output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n            return output\n        else:\n            for mod in self.layers: output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n            return output\n\n\n\nclass TSTEncoderLayer(nn.Module):\n    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=256, store_attn=False,\n                 norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, activation=\"gelu\", res_attention=False, pre_norm=False):\n        super().__init__()\n        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n        d_k = d_model // n_heads if d_k is None else d_k\n        d_v = d_model // n_heads if d_v is None else d_v\n\n        # Multi-Head attention\n        self.res_attention = res_attention\n        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n\n        # Add & Norm\n        self.dropout_attn = nn.Dropout(dropout)\n        if \"batch\" in norm.lower():\n            self.norm_attn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n        else:\n            self.norm_attn = nn.LayerNorm(d_model)\n\n        # Position-wise Feed-Forward\n        self.ff = nn.Sequential(nn.Linear(d_model, d_ff, bias=bias),\n                                get_activation_fn(activation),\n                                nn.Dropout(dropout),\n                                nn.Linear(d_ff, d_model, bias=bias))\n\n        # Add & Norm\n        self.dropout_ffn = nn.Dropout(dropout)\n        if \"batch\" in norm.lower():\n            self.norm_ffn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n        else:\n            self.norm_ffn = nn.LayerNorm(d_model)\n\n        self.pre_norm = pre_norm\n        self.store_attn = store_attn\n\n\n    def forward(self, src:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None) -> Tensor:\n\n        # Multi-Head attention sublayer\n        if self.pre_norm:\n            src = self.norm_attn(src)\n        ## Multi-Head attention\n        if self.res_attention:\n            src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n        else:\n            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n        if self.store_attn:\n            self.attn = attn\n        ## Add & Norm\n        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n        if not self.pre_norm:\n            src = self.norm_attn(src)\n\n        # Feed-forward sublayer\n        if self.pre_norm:\n            src = self.norm_ffn(src)\n        ## Position-wise Feed-Forward\n        src2 = self.ff(src)\n        ## Add & Norm\n        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n        if not self.pre_norm:\n            src = self.norm_ffn(src)\n\n        if self.res_attention:\n            return src, scores\n        else:\n            return src\n\n\n\n\nclass _MultiheadAttention(nn.Module):\n    def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True, lsa=False):\n        \"\"\"Multi Head Attention Layer\n        Input shape:\n            Q:       [batch_size (bs) x max_q_len x d_model]\n            K, V:    [batch_size (bs) x q_len x d_model]\n            mask:    [q_len x q_len]\n        \"\"\"\n        super().__init__()\n        d_k = d_model // n_heads if d_k is None else d_k\n        d_v = d_model // n_heads if d_v is None else d_v\n\n        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n\n        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n\n        # Scaled Dot-Product Attention (multiple heads)\n        self.res_attention = res_attention\n        self.sdp_attn = _ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention, lsa=lsa)\n\n        # Poject output\n        self.to_out = nn.Sequential(nn.Linear(n_heads * d_v, d_model), nn.Dropout(proj_dropout))\n\n\n    def forward(self, Q:Tensor, K:Optional[Tensor]=None, V:Optional[Tensor]=None, prev:Optional[Tensor]=None,\n                key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n\n        bs = Q.size(0)\n        if K is None: K = Q\n        if V is None: V = Q\n\n        # Linear (+ split in multiple heads)\n        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n\n        # Apply Scaled Dot-Product Attention (multiple heads)\n        if self.res_attention:\n            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n        else:\n            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n\n        # back to the original inputs dimensions\n        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n        output = self.to_out(output)\n\n        if self.res_attention: return output, attn_weights, attn_scores\n        else: return output, attn_weights\n\n\nclass _ScaledDotProductAttention(nn.Module):\n    r\"\"\"Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer\n    (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets\n    by Lee et al, 2021)\"\"\"\n\n    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n        super().__init__()\n        self.attn_dropout = nn.Dropout(attn_dropout)\n        self.res_attention = res_attention\n        head_dim = d_model // n_heads\n        self.scale = nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n        self.lsa = lsa\n\n    def forward(self, q:Tensor, k:Tensor, v:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n        '''\n        Input shape:\n            q               : [bs x n_heads x max_q_len x d_k]\n            k               : [bs x n_heads x d_k x seq_len]\n            v               : [bs x n_heads x seq_len x d_v]\n            prev            : [bs x n_heads x q_len x seq_len]\n            key_padding_mask: [bs x seq_len]\n            attn_mask       : [1 x seq_len x seq_len]\n        Output shape:\n            output:  [bs x n_heads x q_len x d_v]\n            attn   : [bs x n_heads x q_len x seq_len]\n            scores : [bs x n_heads x q_len x seq_len]\n        '''\n\n        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n\n        # Add pre-softmax attention scores from the previous layer (optional)\n        if prev is not None: attn_scores = attn_scores + prev\n\n        # Attention mask (optional)\n        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n            if attn_mask.dtype == torch.bool:\n                attn_scores.masked_fill_(attn_mask, -np.inf)\n            else:\n                attn_scores += attn_mask\n\n        # Key padding mask (optional)\n        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n\n        # normalize the attention weights\n        attn_weights = F.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n        attn_weights = self.attn_dropout(attn_weights)\n\n        # compute the new values given the attention weights\n        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n\n        if self.res_attention: return output, attn_weights, attn_scores\n        else: return output, attn_weights\n\n\n\nclass CrossAttentionBlock(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff=2048, dropout=0.1):\n        super().__init__()\n        \n        self.cross_attn = AttentionLayer(\n            FullAttention(mask_flag=False, attention_dropout=dropout, output_attention=False),\n            d_model=d_model,\n            n_heads=n_heads\n        )\n        \n        self.norm1 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout),\n        )\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x, context, attn_mask=None):\n        \"\"\"\n        x: decoder output [B, L_dec, D]\n        context: encoder output [B, L_enc, D]\n        \"\"\"\n        # --- Cross Attention ---\n        residual = x\n        q = self.norm1(x)\n        k = context\n        v = context\n        attn_out, _ = self.cross_attn(q, k, v, attn_mask)  # [B, L_dec, D]\n        x = residual + self.dropout1(attn_out)\n\n        \n        residual = x\n        x = self.norm2(x)\n        x = residual + self.dropout2(self.ff(x))\n\n        return x\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:57:04.975521Z","iopub.execute_input":"2025-10-02T20:57:04.975734Z","iopub.status.idle":"2025-10-02T20:57:05.017516Z","shell.execute_reply.started":"2025-10-02T20:57:04.975717Z","shell.execute_reply":"2025-10-02T20:57:05.016828Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"__all__ = ['PatchTST']\n\n# Cell\nfrom typing import Callable, Optional\nimport torch\nfrom torch import nn\nfrom torch import Tensor\nimport torch.nn.functional as F\nimport numpy as np\n\n\n\nclass PatchTST(nn.Module):\n    def __init__(self, configs, max_seq_len:Optional[int]=1024, d_k:Optional[int]=None, d_v:Optional[int]=None, norm:str='BatchNorm', attn_dropout:float=0., \n                 act:str=\"gelu\", key_padding_mask:bool='auto',padding_var:Optional[int]=None, attn_mask:Optional[Tensor]=None, res_attention:bool=True, \n                 pre_norm:bool=False, store_attn:bool=False, pe:str='zeros', learn_pe:bool=True, pretrain_head:bool=False, head_type = 'flatten', verbose:bool=False, **kwargs):\n        \n        super().__init__()\n        \n        # load parameters\n        c_in = configs.enc_in\n        context_window = configs.seq_len\n        target_window = configs.pred_len\n        \n        n_layers = configs.te_layers\n        n_heads = configs.tn_heads\n        d_model = configs.td_model\n        d_ff = configs.d_ff\n        dropout = configs.dropout\n        fc_dropout = configs.fc_dropout\n        head_dropout = configs.head_dropout\n        \n        individual = configs.individual\n    \n        patch_len = configs.patch_len\n        stride = configs.stride\n        kernel_window = configs.kernel_window\n        padding_patch = configs.padding_patch\n        \n        revin = configs.revin\n        affine = configs.affine\n        subtract_last = configs.subtract_last\n        \n        decomposition = configs.decomposition\n        kernel_size = configs.kernel_size\n        \n        \n        # model\n        self.model = PatchTST_backbone(c_in=c_in, context_window = context_window, target_window=target_window, patch_len=patch_len, stride=stride, kernel_window=kernel_window, \n                              max_seq_len=max_seq_len, n_layers=n_layers, d_model=d_model,\n                              n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n                              dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var, \n                              attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n                              pe=pe, learn_pe=learn_pe, fc_dropout=fc_dropout, head_dropout=head_dropout, padding_patch = padding_patch,\n                              pretrain_head=pretrain_head, head_type=head_type, individual=individual, revin=revin, affine=affine,\n                              subtract_last=subtract_last, verbose=verbose, **kwargs)\n\n    \n    def forward(self, x):           # x: [Batch, Input length, Channel]\n        x = x.permute(0,2,1)    # x: [Batch, Channel, Input length]\n        x = self.model(x)\n        return x","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:57:05.018218Z","iopub.execute_input":"2025-10-02T20:57:05.018689Z","iopub.status.idle":"2025-10-02T20:57:05.037808Z","shell.execute_reply.started":"2025-10-02T20:57:05.018670Z","shell.execute_reply":"2025-10-02T20:57:05.037219Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom math import sqrt\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n        super(EncoderLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.attention = attention\n        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n\n    def forward(self, x, attn_mask=None, tau=None, delta=None):\n        new_x, attn = self.attention(\n            x, x, x,\n            attn_mask=attn_mask,\n            tau=tau, delta=delta\n        )\n        x = x + self.dropout(new_x)\n\n        y = x = self.norm1(x)\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n\n        return self.norm2(x + y), attn\n\n\nclass Encoder(nn.Module):\n    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n        super(Encoder, self).__init__()\n        self.attn_layers = nn.ModuleList(attn_layers)\n        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n        self.norm = norm_layer\n\n    def forward(self, x, attn_mask=None, tau=None, delta=None):\n        # x [B, L, D]\n        attns = []\n        if self.conv_layers is not None:\n            for i, (attn_layer, conv_layer) in enumerate(zip(self.attn_layers, self.conv_layers)):\n                delta = delta if i == 0 else None\n                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                x = conv_layer(x)\n                attns.append(attn)\n            x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n            attns.append(attn)\n        else:\n            for attn_layer in self.attn_layers:\n                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                attns.append(attn)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        return x, attns\n\nclass FullAttention(nn.Module):\n    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n        super(FullAttention, self).__init__()\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n\n    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n        B, L, H, E = queries.shape\n        _, S, _, D = values.shape\n        scale = self.scale or 1. / sqrt(E)\n\n        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n\n        if self.mask_flag:\n            if attn_mask is None:\n                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n\n            scores.masked_fill_(attn_mask.mask, -np.inf)\n\n        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n\n        if self.output_attention:\n            return (V.contiguous(), A)\n        else:\n            return (V.contiguous(), None)\nclass AttentionLayer(nn.Module):\n    def __init__(self, attention, d_model, n_heads, d_keys=None,\n                 d_values=None):\n        super(AttentionLayer, self).__init__()\n\n        d_keys = d_keys or (d_model // n_heads)\n        d_values = d_values or (d_model // n_heads)\n\n        self.inner_attention = attention\n        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n        self.n_heads = n_heads\n\n    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n        B, L, _ = queries.shape\n        _, S, _ = keys.shape\n        H = self.n_heads\n\n        queries = self.query_projection(queries).view(B, L, H, -1)\n        keys = self.key_projection(keys).view(B, S, H, -1)\n        values = self.value_projection(values).view(B, S, H, -1)\n\n        out, attn = self.inner_attention(\n            queries,\n            keys,\n            values,\n            attn_mask,\n            tau=tau,\n            delta=delta\n        )\n        out = out.view(B, L, -1)\n\n        return self.out_projection(out), attn\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:57:05.038607Z","iopub.execute_input":"2025-10-02T20:57:05.038831Z","iopub.status.idle":"2025-10-02T20:57:05.058195Z","shell.execute_reply.started":"2025-10-02T20:57:05.038813Z","shell.execute_reply":"2025-10-02T20:57:05.057396Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport math\n\nclass DataEmbedding_inverted(nn.Module):\n    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n        super(DataEmbedding_inverted, self).__init__()\n        self.value_embedding = nn.Linear(c_in, d_model)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, x_mark):\n        x = x.permute(0, 2, 1)\n        # x: [Batch Variate Time]\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            # the potential to take covariates (e.g. timestamps) as tokens\n            x = self.value_embedding(torch.cat([x, x_mark.permute(0, 2, 1)], 1)) \n        # x: [Batch Variate d_model]\n        return self.dropout(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:57:05.058976Z","iopub.execute_input":"2025-10-02T20:57:05.059244Z","iopub.status.idle":"2025-10-02T20:57:05.074473Z","shell.execute_reply.started":"2025-10-02T20:57:05.059222Z","shell.execute_reply":"2025-10-02T20:57:05.073900Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\nclass iTransformer(nn.Module):\n    \"\"\"\n    Paper link: https://arxiv.org/abs/2310.06625\n    \"\"\"\n\n    def __init__(self, configs):\n        super(iTransformer, self).__init__()\n        self.seq_len = configs.seq_len\n        self.pred_len = configs.pred_len\n        self.output_attention = configs.output_attention\n        self.use_norm = configs.use_norm\n        # Embedding\n        self.enc_embedding = DataEmbedding_inverted(configs.seq_len, configs.cd_model, configs.embed, configs.freq,\n                                                    configs.dropout)\n        self.class_strategy = configs.class_strategy\n\n        self.linear_gate = nn.Linear(2*configs.td_model, configs.cd_model)\n        self.linear_cand = nn.Linear(2*configs.td_model, configs.cd_model)\n        self.gate_norm = nn.LayerNorm(configs.cd_model)\n        \n        # Encoder-only architecture\n        self.encoder = Encoder(\n            [\n                EncoderLayer(\n                    AttentionLayer(\n                        FullAttention(False, configs.factor, attention_dropout=configs.dropout,\n                                      output_attention=configs.output_attention), configs.cd_model, configs.cn_heads),\n                    configs.cd_model,\n                    configs.d_ff,\n                    dropout=configs.dropout,\n                    activation=configs.activation\n                ) for l in range(configs.ce_layers)\n            ],\n            norm_layer=torch.nn.LayerNorm(configs.cd_model)\n        )\n        self.projector = nn.Linear(configs.cd_model, configs.pred_len, bias=True)\n\n    def forecast(self, x_enc, H, x_mark_enc, x_dec, x_mark_dec):\n        if self.use_norm:\n            # Normalization from Non-stationary Transformer\n            means = x_enc.mean(1, keepdim=True).detach()\n            x_enc = x_enc - means\n            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n            x_enc /= stdev\n\n        _, _, N = x_enc.shape # B L N\n        # B: batch_size;    E: d_model; \n        # L: seq_len;       S: pred_len;\n        # N: number of variate (tokens), can also includes covariates\n\n        # Embedding\n        # B L N -> B N E                (B L N -> B L E in the vanilla Transformer)\n        enc_out = self.enc_embedding(x_enc, x_mark_enc) # covariates (e.g timestamp) can be also embedded as tokens\n\n        #input gating\n        concat = torch.cat([H,enc_out], dim=-1)\n        i_t = torch.sigmoid(self.linear_gate(concat))\n        cand = torch.tanh(self.linear_cand(concat))\n        enc_out = self.gate_norm(enc_out + i_t * cand)\n        \n        # B N E -> B N E                (B L E -> B L E in the vanilla Transformer)\n        # the dimensions of embedded time series has been inverted, and then processed by native attn, layernorm and ffn modules\n        enc_out, attns = self.encoder(enc_out, attn_mask=None)\n\n        # B N E -> B N S -> B S N \n        dec_out = self.projector(enc_out).permute(0, 2, 1)[:, :, :N] # filter the covariates\n\n        if self.use_norm:\n            # De-Normalization from Non-stationary Transformer\n            dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))\n            dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))\n\n        return dec_out, attns\n\n\n    def forward(self, x_enc, H, x_mark_enc=None, x_dec=None, x_mark_dec=None, mask=None):\n        dec_out, attns = self.forecast(x_enc, H, x_mark_enc, x_dec, x_mark_dec)\n        \n        if self.output_attention:\n            return dec_out[:, -self.pred_len:, :], attns\n        else:\n            return dec_out[:, -self.pred_len:, :]  # [B, L, D]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:57:05.075331Z","iopub.execute_input":"2025-10-02T20:57:05.075981Z","iopub.status.idle":"2025-10-02T20:57:05.090149Z","shell.execute_reply.started":"2025-10-02T20:57:05.075956Z","shell.execute_reply":"2025-10-02T20:57:05.089456Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,configs):\n        super().__init__()\n        self.encoder1 = PatchTST(configs)\n        self.encoder2 = iTransformer(configs)\n        \n\n    def forward(self,x):\n        enc1_out=self.encoder1(x)\n        enc2_out=self.encoder2(x,enc1_out)\n        \n        return enc2_out\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:57:05.090955Z","iopub.execute_input":"2025-10-02T20:57:05.091445Z","iopub.status.idle":"2025-10-02T20:57:05.105679Z","shell.execute_reply.started":"2025-10-02T20:57:05.091420Z","shell.execute_reply":"2025-10-02T20:57:05.105058Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        # Model architecture parameters\n        self.enc_in = 170              # Number of input channels/features\n        self.seq_len = 96            # Input sequence length\n        self.individual = False \n        self.pred_len =  24          # Prediction length\n        self.te_layers = 4           # Number of encoder layers\n        self.tn_heads = 4             # Number of attention heads\n        self.ce_layers = 6           # Number of encoder layers\n        self.cn_heads = 4             # Number of attention heads\n        self.td_model = 128           # Model dimension\n        self.cd_model = 128           # Model dimension\n        self.d_ff = 512              # Feed-forward dimension\n        self.dropout = 0.2           # Dropout rate\n        self.fc_dropout = 0.1        # Final fully connected dropout\n        self.head_dropout = 0.1      # Head dropout rate\n        self.output_attention = False\n        self.use_norm = True\n        self.embed = 'fixed'\n        self.freq = 'h'\n        self.class_strategy = 'none'\n        self.factor = 5\n        self.activation = 'relu'\n        \n        # Patch parameters\n        self.patch_len = 16          # Length of each patch\n        self.stride = 16              # Stride for patch creation\n        self.kernel_window = 7         #for pre conv\n        self.padding_patch = 'end'   # Patch padding strategy\n        \n        # Normalization parameters\n        self.revin = True            # Enable RevIN normalization\n        self.affine = True           # Affine transformation in RevIN\n        self.subtract_last = False   # Subtract last value for normalization\n        \n        # Decomposition parameters\n        self.decomposition = False   # Enable series decomposition\n        self.kernel_size = 25        # Kernel size for decomposition\n\n# Create configuration instance\nconfigs = Config()\nmodel = Model(configs).to('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:57:05.107463Z","iopub.execute_input":"2025-10-02T20:57:05.107665Z","iopub.status.idle":"2025-10-02T20:57:05.153747Z","shell.execute_reply.started":"2025-10-02T20:57:05.107649Z","shell.execute_reply":"2025-10-02T20:57:05.153156Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom tqdm import tqdm\n\n# ---- Loss & Metrics ----\nmse_loss = nn.MSELoss()\n\ndef mae_loss(pred, true):\n    return torch.mean(torch.abs(pred - true))\n\n# ---- Train & Evaluate ----\ndef train(model, train_loader, optimizer, device, pred_len):\n    model.train()\n    total_loss = 0\n    for seq_x, seq_y, seq_x_mark, seq_y_mark in tqdm(train_loader, desc=\"Training\", leave=False):\n        seq_x = seq_x.to(device).float()\n        target = seq_y[:, -pred_len:, :].to(device).float()  # take last pred_len steps\n\n        optimizer.zero_grad()\n        outputs = model(seq_x)\n        loss = mse_loss(outputs, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(train_loader)\n\n\ndef evaluate(model, val_loader, device, pred_len):\n    model.eval()\n    total_mse, total_mae = 0, 0\n    with torch.no_grad():\n        for seq_x, seq_y, seq_x_mark, seq_y_mark in tqdm(val_loader, desc=\"Validating\", leave=False):\n            seq_x = seq_x.to(device).float()\n            target = seq_y[:, -pred_len:, :].to(device).float()\n\n            outputs = model(seq_x)\n            total_mse += mse_loss(outputs, target).item()\n            total_mae += mae_loss(outputs, target).item()\n\n    return total_mse / len(val_loader), total_mae / len(val_loader)\n\n\ndef test(model, test_loader, device, pred_len):\n    model.eval()\n    total_mse, total_mae = 0, 0\n    preds, trues = [], []\n    with torch.no_grad():\n        for seq_x, seq_y, seq_x_mark, seq_y_mark in tqdm(test_loader, desc=\"Testing\", leave=False):\n            seq_x = seq_x.to(device).float()\n            target = seq_y[:, -pred_len:, :].to(device).float()\n\n            outputs = model(seq_x)\n            total_mse += mse_loss(outputs, target).item()\n            total_mae += mae_loss(outputs, target).item()\n\n            preds.append(outputs.cpu().numpy())\n            trues.append(target.cpu().numpy())\n\n    preds = np.concatenate(preds, axis=0)\n    trues = np.concatenate(trues, axis=0)\n    return total_mse / len(test_loader), total_mae / len(test_loader), preds, trues\n\n\n# ---- Main Training Loop ----\ndef train_model(model, train_loader, val_loader, test_loader, pred_len, epochs=40, lr=1e-4, patience=5, device='cuda'):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    \n    best_val_loss = float('inf')\n    best_model = None\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        \n        train_loss = train(model, train_loader, optimizer, device, pred_len)\n        val_mse, val_mae = evaluate(model, val_loader, device, pred_len)\n        scheduler.step()\n\n        print(f\"Train Loss: {train_loss:.6f} | Val MSE: {val_mse:.6f} | Val MAE: {val_mae:.6f}\")\n\n        # Early stopping\n        if val_mse < best_val_loss:\n            best_val_loss = val_mse\n            best_model = model.state_dict()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    # Load best model\n    model.load_state_dict(best_model)\n\n    # Final test\n    test_mse, test_mae, preds, trues = test(model, test_loader, device, pred_len)\n    print(f\"\\nTest MSE: {test_mse:.6f} | Test MAE: {test_mae:.6f}\")\n    return model, preds, trues\n\n\n# ---- Run training ----\ntrain_model(model, train_loader, val_loader, test_loader, pred_len=size[2])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:57:05.154529Z","iopub.execute_input":"2025-10-02T20:57:05.154785Z","iopub.status.idle":"2025-10-02T21:27:43.825404Z","shell.execute_reply.started":"2025-10-02T20:57:05.154761Z","shell.execute_reply":"2025-10-02T21:27:43.824712Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.271871 | Val MSE: 0.200087 | Val MAE: 0.299231\n\nEpoch 2/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.161596 | Val MSE: 0.221839 | Val MAE: 0.326232\n\nEpoch 3/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.148621 | Val MSE: 0.159973 | Val MAE: 0.260660\n\nEpoch 4/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.136488 | Val MSE: 0.159732 | Val MAE: 0.262680\n\nEpoch 5/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.130208 | Val MSE: 0.152419 | Val MAE: 0.255332\n\nEpoch 6/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.125430 | Val MSE: 0.147906 | Val MAE: 0.251907\n\nEpoch 7/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.120806 | Val MSE: 0.159681 | Val MAE: 0.265214\n\nEpoch 8/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.117428 | Val MSE: 0.143158 | Val MAE: 0.243686\n\nEpoch 9/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.114046 | Val MSE: 0.135541 | Val MAE: 0.235006\n\nEpoch 10/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.111561 | Val MSE: 0.133725 | Val MAE: 0.234391\n\nEpoch 11/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.108950 | Val MSE: 0.129191 | Val MAE: 0.230416\n\nEpoch 12/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.106640 | Val MSE: 0.135147 | Val MAE: 0.238445\n\nEpoch 13/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.104573 | Val MSE: 0.125420 | Val MAE: 0.225384\n\nEpoch 14/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.102842 | Val MSE: 0.130556 | Val MAE: 0.232234\n\nEpoch 15/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.101382 | Val MSE: 0.126486 | Val MAE: 0.228124\n\nEpoch 16/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.099605 | Val MSE: 0.122819 | Val MAE: 0.222479\n\nEpoch 17/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.098559 | Val MSE: 0.125534 | Val MAE: 0.226658\n\nEpoch 18/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.097315 | Val MSE: 0.119590 | Val MAE: 0.219780\n\nEpoch 19/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.096585 | Val MSE: 0.120575 | Val MAE: 0.221358\n\nEpoch 20/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.095275 | Val MSE: 0.118258 | Val MAE: 0.217852\n\nEpoch 21/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.094647 | Val MSE: 0.117446 | Val MAE: 0.217264\n\nEpoch 22/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.093950 | Val MSE: 0.116497 | Val MAE: 0.215441\n\nEpoch 23/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.093061 | Val MSE: 0.116645 | Val MAE: 0.216056\n\nEpoch 24/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.092435 | Val MSE: 0.116025 | Val MAE: 0.215556\n\nEpoch 25/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.091823 | Val MSE: 0.115612 | Val MAE: 0.215662\n\nEpoch 26/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.091530 | Val MSE: 0.115902 | Val MAE: 0.216241\n\nEpoch 27/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.090989 | Val MSE: 0.115512 | Val MAE: 0.215059\n\nEpoch 28/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.090808 | Val MSE: 0.115228 | Val MAE: 0.214891\n\nEpoch 29/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.090370 | Val MSE: 0.114728 | Val MAE: 0.214197\n\nEpoch 30/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.089977 | Val MSE: 0.114474 | Val MAE: 0.213653\n\nEpoch 31/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.089828 | Val MSE: 0.114132 | Val MAE: 0.213382\n\nEpoch 32/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.089563 | Val MSE: 0.114358 | Val MAE: 0.213853\n\nEpoch 33/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.089307 | Val MSE: 0.113907 | Val MAE: 0.213637\n\nEpoch 34/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.089138 | Val MSE: 0.113444 | Val MAE: 0.212497\n\nEpoch 35/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.089131 | Val MSE: 0.114646 | Val MAE: 0.213877\n\nEpoch 36/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.088897 | Val MSE: 0.113672 | Val MAE: 0.212884\n\nEpoch 37/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.089117 | Val MSE: 0.113367 | Val MAE: 0.212443\n\nEpoch 38/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.088948 | Val MSE: 0.113191 | Val MAE: 0.212385\n\nEpoch 39/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.088941 | Val MSE: 0.113586 | Val MAE: 0.212801\n\nEpoch 40/40\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.089034 | Val MSE: 0.113687 | Val MAE: 0.212885\n","output_type":"stream"},{"name":"stderr","text":"                                                          ","output_type":"stream"},{"name":"stdout","text":"\nTest MSE: 0.099330 | Test MAE: 0.204576\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"(Model(\n   (encoder1): PatchTST(\n     (model): PatchTST_backbone(\n       (revin_layer): RevIN()\n       (channel_wise_conv): ChannelWiseConv(\n         (conv): Conv1d(170, 170, kernel_size=(7,), stride=(1,), padding=(3,), groups=170)\n       )\n       (post_conv_norm): BatchNorm1d(170, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (padding_patch_layer): ReplicationPad1d((0, 16))\n       (backbone): TSTiEncoder(\n         (W_P): Linear(in_features=16, out_features=128, bias=True)\n         (dropout): Dropout(p=0.2, inplace=False)\n         (encoder): TSTEncoder(\n           (layers): ModuleList(\n             (0-3): 4 x TSTEncoderLayer(\n               (self_attn): _MultiheadAttention(\n                 (W_Q): Linear(in_features=128, out_features=128, bias=True)\n                 (W_K): Linear(in_features=128, out_features=128, bias=True)\n                 (W_V): Linear(in_features=128, out_features=128, bias=True)\n                 (sdp_attn): _ScaledDotProductAttention(\n                   (attn_dropout): Dropout(p=0.0, inplace=False)\n                 )\n                 (to_out): Sequential(\n                   (0): Linear(in_features=128, out_features=128, bias=True)\n                   (1): Dropout(p=0.2, inplace=False)\n                 )\n               )\n               (dropout_attn): Dropout(p=0.2, inplace=False)\n               (norm_attn): Sequential(\n                 (0): Transpose()\n                 (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                 (2): Transpose()\n               )\n               (ff): Sequential(\n                 (0): Linear(in_features=128, out_features=512, bias=True)\n                 (1): GELU(approximate='none')\n                 (2): Dropout(p=0.2, inplace=False)\n                 (3): Linear(in_features=512, out_features=128, bias=True)\n               )\n               (dropout_ffn): Dropout(p=0.2, inplace=False)\n               (norm_ffn): Sequential(\n                 (0): Transpose()\n                 (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                 (2): Transpose()\n               )\n             )\n           )\n         )\n       )\n       (temporal_context_linear): Linear(in_features=896, out_features=128, bias=True)\n     )\n   )\n   (encoder2): iTransformer(\n     (enc_embedding): DataEmbedding_inverted(\n       (value_embedding): Linear(in_features=96, out_features=128, bias=True)\n       (dropout): Dropout(p=0.2, inplace=False)\n     )\n     (linear_gate): Linear(in_features=256, out_features=128, bias=True)\n     (linear_cand): Linear(in_features=256, out_features=128, bias=True)\n     (gate_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n     (encoder): Encoder(\n       (attn_layers): ModuleList(\n         (0-5): 6 x EncoderLayer(\n           (attention): AttentionLayer(\n             (inner_attention): FullAttention(\n               (dropout): Dropout(p=0.2, inplace=False)\n             )\n             (query_projection): Linear(in_features=128, out_features=128, bias=True)\n             (key_projection): Linear(in_features=128, out_features=128, bias=True)\n             (value_projection): Linear(in_features=128, out_features=128, bias=True)\n             (out_projection): Linear(in_features=128, out_features=128, bias=True)\n           )\n           (conv1): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n           (conv2): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n           (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n           (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n           (dropout): Dropout(p=0.2, inplace=False)\n         )\n       )\n       (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n     )\n     (projector): Linear(in_features=128, out_features=24, bias=True)\n   )\n ),\n array([[[-5.0013578e-01,  1.2421733e-01,  4.6627647e-01, ...,\n          -5.2275419e-02, -5.3892094e-01, -1.9939339e-01],\n         [-5.2548599e-01,  1.1852425e-01,  4.4603866e-01, ...,\n          -7.2580338e-02, -5.7338709e-01, -2.5343478e-01],\n         [-5.3752273e-01,  8.9515984e-02,  4.3446422e-01, ...,\n          -9.3654633e-02, -6.2542456e-01, -3.0047393e-01],\n         ...,\n         [-1.0357194e+00, -4.8502231e-01,  6.6250026e-02, ...,\n          -6.1814040e-01, -1.4170537e+00, -1.1498904e+00],\n         [-1.0485554e+00, -5.0515938e-01,  5.4286659e-02, ...,\n          -6.1818153e-01, -1.4259949e+00, -1.1971874e+00],\n         [-1.0579495e+00, -5.3054726e-01,  3.3946812e-02, ...,\n          -6.3739043e-01, -1.4469728e+00, -1.1759801e+00]],\n \n        [[-5.2099770e-01,  1.4913976e-01,  4.8836648e-01, ...,\n          -4.7467589e-02, -5.9318650e-01, -2.3772657e-01],\n         [-5.5001467e-01,  1.3459527e-01,  4.5680076e-01, ...,\n          -7.3944986e-02, -6.5484536e-01, -2.9878187e-01],\n         [-5.7119125e-01,  1.0693365e-01,  4.4420704e-01, ...,\n          -9.5029593e-02, -7.2197664e-01, -3.5279822e-01],\n         ...,\n         [-1.1327944e+00, -4.6140301e-01,  5.2245021e-02, ...,\n          -5.8893394e-01, -1.6843646e+00, -1.2417790e+00],\n         [-1.1589386e+00, -4.9078321e-01,  3.7419975e-02, ...,\n          -6.0579181e-01, -1.7230558e+00, -1.2700151e+00],\n         [-1.1562545e+00, -5.1269960e-01,  1.7734170e-02, ...,\n          -6.1632025e-01, -1.7460368e+00, -1.2733094e+00]],\n \n        [[-6.6154760e-01,  4.4033349e-02,  4.0782508e-01, ...,\n          -8.5604787e-03, -7.0513916e-01, -8.7728024e-02],\n         [-7.0000523e-01,  2.0298541e-02,  3.8581142e-01, ...,\n          -2.6983678e-02, -7.6043880e-01, -1.6670239e-01],\n         [-7.1934992e-01, -2.5089741e-02,  3.7024251e-01, ...,\n          -4.5916796e-02, -8.2742631e-01, -2.0588028e-01],\n         ...,\n         [-1.2948916e+00, -6.7009330e-01,  1.5005529e-02, ...,\n          -5.1888716e-01, -1.7620181e+00, -1.1003535e+00],\n         [-1.3124349e+00, -6.9678640e-01,  1.0589957e-03, ...,\n          -5.1734924e-01, -1.7797977e+00, -1.1358674e+00],\n         [-1.3321939e+00, -7.2306037e-01, -4.4316053e-03, ...,\n          -5.3017128e-01, -1.8111736e+00, -1.1345747e+00]],\n \n        ...,\n \n        [[-7.3092341e-01, -4.1461051e-02, -2.4143028e-01, ...,\n          -7.0946562e-01, -6.1040616e-01, -4.9577242e-01],\n         [-7.5037181e-01, -5.2277029e-02, -2.5118005e-01, ...,\n          -7.5412154e-01, -6.3337743e-01, -5.3182310e-01],\n         [-7.7920949e-01, -8.2118690e-02, -2.7898979e-01, ...,\n          -8.0658162e-01, -6.8220603e-01, -5.6143945e-01],\n         ...,\n         [-1.2398502e+00, -5.4892844e-01, -6.3454008e-01, ...,\n          -1.4286759e+00, -1.2881286e+00, -1.1391242e+00],\n         [-1.2463219e+00, -5.7829422e-01, -6.5124118e-01, ...,\n          -1.4376333e+00, -1.3016365e+00, -1.1725843e+00],\n         [-1.2597556e+00, -5.9069735e-01, -6.6299725e-01, ...,\n          -1.4737227e+00, -1.3127922e+00, -1.1871941e+00]],\n \n        [[-8.2471704e-01, -5.7443559e-02, -2.6496625e-01, ...,\n          -7.3479587e-01, -6.9628900e-01, -6.2903339e-01],\n         [-8.4434545e-01, -7.1159184e-02, -2.8043151e-01, ...,\n          -7.7178246e-01, -7.1073395e-01, -6.5122491e-01],\n         [-8.6873686e-01, -9.9380136e-02, -3.0366528e-01, ...,\n          -8.2247633e-01, -7.5391430e-01, -6.9855398e-01],\n         ...,\n         [-1.3592433e+00, -5.6411803e-01, -6.5055776e-01, ...,\n          -1.4117873e+00, -1.3592401e+00, -1.3327940e+00],\n         [-1.3796493e+00, -5.9808898e-01, -6.7068183e-01, ...,\n          -1.4217660e+00, -1.3639605e+00, -1.3529403e+00],\n         [-1.3850586e+00, -6.0948861e-01, -6.8363678e-01, ...,\n          -1.4434035e+00, -1.3894520e+00, -1.3855600e+00]],\n \n        [[-8.4920418e-01, -1.0190880e-01, -2.9040551e-01, ...,\n          -7.8102887e-01, -7.1536744e-01, -6.7187297e-01],\n         [-8.7211144e-01, -1.0543215e-01, -3.0358547e-01, ...,\n          -8.1398821e-01, -7.4119759e-01, -7.0098710e-01],\n         [-8.9491045e-01, -1.2747723e-01, -3.2736081e-01, ...,\n          -8.5687256e-01, -7.8435278e-01, -7.5013959e-01],\n         ...,\n         [-1.4182976e+00, -5.3266138e-01, -6.7917061e-01, ...,\n          -1.4309957e+00, -1.4459676e+00, -1.4230531e+00],\n         [-1.4320452e+00, -5.5285972e-01, -6.9578195e-01, ...,\n          -1.4466758e+00, -1.4623109e+00, -1.4556912e+00],\n         [-1.4432169e+00, -5.7507557e-01, -7.1235180e-01, ...,\n          -1.4801691e+00, -1.4937332e+00, -1.4865135e+00]]], dtype=float32),\n array([[[-0.51076216,  0.17354617,  0.58564174, ...,  0.187305  ,\n          -0.5274224 , -0.09545432],\n         [-0.7027069 ,  0.03559164,  0.2949182 , ...,  0.43928066,\n          -0.8222313 ,  0.29376948],\n         [-0.5306185 , -0.03667026,  0.38628846, ...,  0.32298422,\n          -0.7239617 , -0.31786793],\n         ...,\n         [-1.1660217 , -0.654181  , -0.63539714, ..., -0.87874585,\n          -1.5252371 , -0.8739019 ],\n         [-1.0733588 , -0.81184334, -0.6437035 , ..., -0.85936314,\n          -1.4950002 , -0.92950535],\n         [-1.099834  , -0.59505767, -0.48588216, ..., -0.8012149 ,\n          -1.5857106 , -1.3187292 ]],\n \n        [[-0.7027069 ,  0.03559164,  0.2949182 , ...,  0.43928066,\n          -0.8222313 ,  0.29376948],\n         [-0.5306185 , -0.03667026,  0.38628846, ...,  0.32298422,\n          -0.7239617 , -0.31786793],\n         [-0.5504749 , -0.00382395,  0.28661183, ...,  0.2260705 ,\n          -0.7164025 ,  0.12695928],\n         ...,\n         [-1.0733588 , -0.81184334, -0.6437035 , ..., -0.85936314,\n          -1.4950002 , -0.92950535],\n         [-1.099834  , -0.59505767, -0.48588216, ..., -0.8012149 ,\n          -1.5857106 , -1.3187292 ],\n         [-1.3910605 , -0.6213347 , -0.718461  , ..., -0.7333753 ,\n          -1.5554739 , -1.3187292 ]],\n \n        [[-0.5306185 , -0.03667026,  0.38628846, ...,  0.32298422,\n          -0.7239617 , -0.31786793],\n         [-0.5504749 , -0.00382395,  0.28661183, ...,  0.2260705 ,\n          -0.7164025 ,  0.12695928],\n         [-0.4578119 ,  0.03559164,  0.35306293, ..., -0.03559655,\n          -0.5501    , -0.42907473],\n         ...,\n         [-1.099834  , -0.59505767, -0.48588216, ..., -0.8012149 ,\n          -1.5857106 , -1.3187292 ],\n         [-1.3910605 , -0.6213347 , -0.718461  , ..., -0.7333753 ,\n          -1.5554739 , -1.3187292 ],\n         [-1.3381101 , -0.778997  , -0.8430568 , ..., -0.61707884,\n          -1.4874411 , -1.2075223 ]],\n \n        ...,\n \n        [[-0.8417013 , -0.04323953, -0.3363672 , ..., -0.8109063 ,\n          -0.8449089 , -0.6514883 ],\n         [-0.80198866, -0.13520922, -0.2616097 , ..., -0.7721408 ,\n          -0.7088433 , -0.37347132],\n         [-1.0799775 , -0.154917  , -0.46926937, ..., -0.7915235 ,\n          -0.95829695, -0.37347132],\n         ...,\n         [-1.344729  , -1.0877523 , -0.80983126, ..., -0.95627683,\n          -1.5327963 , -1.3743325 ],\n         [-1.3248726 , -1.1403065 , -0.9095079 , ..., -1.062882  ,\n          -1.5554739 , -1.3187292 ],\n         [-1.4175355 , -1.1862913 , -1.0590229 , ..., -1.4699196 ,\n          -1.6008291 , -1.2631258 ]],\n \n        [[-0.80198866, -0.13520922, -0.2616097 , ..., -0.7721408 ,\n          -0.7088433 , -0.37347132],\n         [-1.0799775 , -0.154917  , -0.46926937, ..., -0.7915235 ,\n          -0.95829695, -0.37347132],\n         [-0.80198866, -0.0826551 , -0.4526566 , ..., -0.830289  ,\n          -0.98853374, -0.6514883 ],\n         ...,\n         [-1.3248726 , -1.1403065 , -0.9095079 , ..., -1.062882  ,\n          -1.5554739 , -1.3187292 ],\n         [-1.4175355 , -1.1862913 , -1.0590229 , ..., -1.4699196 ,\n          -1.6008291 , -1.2631258 ],\n         [-1.5366737 , -1.2585533 , -0.9427334 , ..., -0.98535097,\n          -1.6461843 , -1.3187292 ]],\n \n        [[-1.0799775 , -0.154917  , -0.46926937, ..., -0.7915235 ,\n          -0.95829695, -0.37347132],\n         [-0.80198866, -0.0826551 , -0.4526566 , ..., -0.830289  ,\n          -0.98853374, -0.6514883 ],\n         [-0.72256327, -0.41111827, -0.2699161 , ..., -1.0241164 ,\n          -0.89782333, -0.31786793],\n         ...,\n         [-1.4175355 , -1.1862913 , -1.0590229 , ..., -1.4699196 ,\n          -1.6008291 , -1.2631258 ],\n         [-1.5366737 , -1.2585533 , -0.9427334 , ..., -0.98535097,\n          -1.6461843 , -1.3187292 ],\n         [-1.5366737 , -1.251984  , -0.9427334 , ..., -1.1210302 ,\n          -1.7595724 , -1.0963155 ]]], dtype=float32))"},"metadata":{}}],"execution_count":24}]}